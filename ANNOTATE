--------------------------------------------------------------------------------
Profile data file 'callgrind.out.8927' (creator: callgrind-3.9.0)
--------------------------------------------------------------------------------
I1 cache: 
D1 cache: 
LL cache: 
Timerange: Basic block 0 - 622613868
Trigger: Program termination
Profiled target:  ./um midmark.um (PID 8927, part 1)
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Thresholds:       99
Include dirs:     
User annotated:   
Auto-annotation:  on

--------------------------------------------------------------------------------
           Ir 
--------------------------------------------------------------------------------
5,249,014,244  PROGRAM TOTALS

--------------------------------------------------------------------------------
           Ir  file:function
--------------------------------------------------------------------------------
3,934,381,548  um.c:main [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
  272,458,256  um.c:exe_segload [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
  260,429,600  um.c:exe_loadval [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
  145,806,732  um.c:exe_segstore [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
  111,691,602  /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:calloc [/lib64/libc-2.12.so]
  104,358,742  /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_free [/lib64/libc-2.12.so]
   98,422,668  /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_malloc [/lib64/libc-2.12.so]
   73,171,495  um.c:exe_map [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
   46,424,417  um.c:exe_jump [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
   43,711,528  um.c:exe_unmap [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
   42,958,900  um.c:exe_nand [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
   28,789,664  um.c:exe_move [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
   28,269,360  um.c:exe_add [/h/sschoe05/Desktop/comp40-2/hw6/Universal-Machine/um]
   25,457,444  /usr/src/debug////////glibc-2.12-2-gc4ccff1/string/../sysdeps/x86_64/memset.S:__GI_memset [/lib64/libc-2.12.so]

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/src/debug////////glibc-2.12-2-gc4ccff1/string/../sysdeps/x86_64/memset.S
--------------------------------------------------------------------------------
     Ir 

-- line 36 ----------------------------------------
      .  #if defined PIC && !defined NOT_IN_libc
      .  ENTRY_CHK (__memset_chk)
      .  	cmpq	%rdx, %rcx
      .  	jb	HIDDEN_JUMPTARGET (__chk_fail)
      .  END_CHK (__memset_chk)
      .  #endif
      .  ENTRY (memset)
      .  L(memset_entry):
600,494  	cmp    $0x1,%rdx
600,494  	mov    %rdi,%rax	/* memset returns the dest address.  */
600,494  	jne    L(ck2)
      .  	mov    %sil,(%rdi)
      .  	retq
      .  L(ck2):
600,494  	mov    $0x101010101010101,%r9
600,494  	mov    %rdx,%r8
600,494  	movzbq %sil,%rdx
600,494  	imul   %r9,%rdx
      .  L(now_dw_aligned):
600,494  	cmp    $0x90,%r8
600,494  	jg     L(ck_mem_ops_method)
      .  L(now_dw_aligned_small):
367,472  	add    %r8,%rdi
      .  #ifndef PIC
      .  	lea    L(setPxQx)(%rip),%r11
      .  	jmpq   *(%r11,%r8,8)
      .  #else
367,472  	lea    L(Got0)(%rip),%r11
367,472  	lea    L(setPxQx)(%rip),%rcx
367,472  	movswq (%rcx,%r8,2),%rcx
367,472  	lea    (%rcx,%r11,1),%r11
367,472  	jmpq   *%r11
      .  #endif
      .  
      .  L(Got0):
      .  	retq
      .  
      .  	.pushsection .rodata
      .  	.balign     16
      .  #ifndef PIC
-- line 75 ----------------------------------------
-- line 314 ----------------------------------------
      .  L(P1Q3): mov    %rdx,-0x19(%rdi)
      .  L(P1Q2): mov    %rdx,-0x11(%rdi)
      .  L(P1Q1): mov    %rdx,-0x9(%rdi)
      .  L(P1Q0): mov    %dl,-0x1(%rdi)
      .  		retq
      .  
      .  	.balign     16
      .  L(P0QI): mov    %rdx,-0x90(%rdi)
  1,289  L(P0QH): mov    %rdx,-0x88(%rdi)
      .  #		   .balign     16
  1,289  L(P0QG): mov    %rdx,-0x80(%rdi)
 37,074  L(P0QF): mov    %rdx,-0x78(%rdi)
 37,074  L(P0QE): mov    %rdx,-0x70(%rdi)
356,555  L(P0QD): mov    %rdx,-0x68(%rdi)
356,555  L(P0QC): mov    %rdx,-0x60(%rdi)
367,465  L(P0QB): mov    %rdx,-0x58(%rdi)
367,465  L(P0QA): mov    %rdx,-0x50(%rdi)
367,465  L(P0Q9): mov    %rdx,-0x48(%rdi)
367,466  L(P0Q8): mov    %rdx,-0x40(%rdi)
367,466  L(P0Q7): mov    %rdx,-0x38(%rdi)
367,466  L(P0Q6): mov    %rdx,-0x30(%rdi)
367,466  L(P0Q5): mov    %rdx,-0x28(%rdi)
367,466  L(P0Q4): mov    %rdx,-0x20(%rdi)
367,466  L(P0Q3): mov    %rdx,-0x18(%rdi)
367,469  L(P0Q2): mov    %rdx,-0x10(%rdi)
367,471  L(P0Q1): mov    %rdx,-0x8(%rdi)
367,471  L(P0Q0): retq
      .  
      .  
      .  	.balign     16
      .  #ifdef USE_EXTRA_TABLE
      .  L(P2QI): mov    %rdx,-0x92(%rdi)
      .  #endif
      .  L(P2QH): mov    %rdx,-0x8a(%rdi)
      .  L(P2QG): mov    %rdx,-0x82(%rdi)
-- line 348 ----------------------------------------
-- line 408 ----------------------------------------
      .  L(P4Q8): mov    %rdx,-0x44(%rdi)
      .  L(P4Q7): mov    %rdx,-0x3c(%rdi)
      .  L(P4Q6): mov    %rdx,-0x34(%rdi)
      .  L(P4Q5): mov    %rdx,-0x2c(%rdi)
      .  L(P4Q4): mov    %rdx,-0x24(%rdi)
      .  L(P4Q3): mov    %rdx,-0x1c(%rdi)
      .  L(P4Q2): mov    %rdx,-0x14(%rdi)
      .  L(P4Q1): mov    %rdx,-0xc(%rdi)
      2  L(P4Q0): mov    %edx,-0x4(%rdi)
      2  		retq
      .  
      .  	.balign     16
      .  #if defined(USE_EXTRA_TABLE)
      .  L(P5QI): mov    %rdx,-0x95(%rdi)
      .  #endif
      .  L(P5QH): mov    %rdx,-0x8d(%rdi)
      .  L(P5QG): mov    %rdx,-0x85(%rdi)
      .  #		   .balign     16
-- line 425 ----------------------------------------
-- line 496 ----------------------------------------
      .  		retq
      .  
      .  	.balign     16
      .  L(ck_mem_ops_method):
      .  
      .  # align to 16 byte boundary first
      .  	#test $0xf,%rdi
      .  	#jz L(aligned_now)
233,022  	mov    $0x10,%r10
233,022  	mov    %rdi,%r9
233,022  	and    $0xf,%r9
233,022  	sub    %r9,%r10
233,022  	and    $0xf,%r10
233,022  	add    %r10,%rdi
233,022  	sub    %r10,%r8
      .  #ifndef PIC
      .  	lea    L(AliPxQx)(%rip),%r11
      .  	jmpq   *(%r11,%r10,8)
      .  #else
233,022  	lea    L(aligned_now)(%rip), %r11
233,022  	lea    L(AliPxQx)(%rip),%rcx
233,022  	movswq (%rcx,%r10,2),%rcx
233,022  	lea    (%rcx,%r11,1),%r11
233,022  	jmpq   *%r11
      .  #endif
      .  
      .  	.pushsection .rodata
      .  	.balign     16
      .  #ifndef PIC
      .  L(AliPxQx):
      .  	.quad       L(aligned_now), L(A1Q0), L(A2Q0), L(A3Q0)
      .  	.quad	    L(A4Q0), L(A5Q0), L(A6Q0), L(A7Q0)
-- line 527 ----------------------------------------
-- line 547 ----------------------------------------
      .  	.short     L(A6Q1)-L(aligned_now)
      .  	.short     L(A7Q1)-L(aligned_now)
      .  #endif
      .  	.popsection
      .  
      .  	.balign     16
      .  L(A5Q1):    mov    %dl,-0xd(%rdi)
      .  L(A4Q1):    mov    %edx,-0xc(%rdi)
      1  L(A0Q1):    mov    %rdx,-0x8(%rdi)
      1  L(A0Q0):    jmp     L(aligned_now)
      .  
      .  	.balign     16
      .  L(A1Q1):   mov    %dl,-0x9(%rdi)
      .  	mov    %rdx,-0x8(%rdi)
      .  	jmp    L(aligned_now)
      .  
      .  	.balign     16
      .  L(A1Q0):   mov    %dl,-0x1(%rdi)
-- line 564 ----------------------------------------
-- line 585 ----------------------------------------
      .  L(A6Q1):    mov    %dx,-0xe(%rdi)
      .  	mov    %edx,-0xc(%rdi)
      .  	mov    %rdx,-0x8(%rdi)
      .  	jmp    L(aligned_now)
      .  
      .  	.balign     16
      .  L(A7Q0):    mov    %dl,-0x7(%rdi)
      .  L(A6Q0):    mov    %dx,-0x6(%rdi)
      1  	mov    %edx,-0x4(%rdi)
      .  
      .  #ifndef USE_MULTIARCH
      .  	jmp    L(aligned_now)
      .  
      .  L(SSE_pre):
      .  #else
      .  L(aligned_now):
      .  #endif
      .  #if !defined USE_MULTIARCH || defined USE_SSE2
      .  	 # fill RegXMM0 with the pattern
233,021  	 movd   %rdx,%xmm0
233,021  	 punpcklqdq %xmm0,%xmm0
      .  
233,021  	 cmp    $0xb0,%r8 # 176
233,021  	 jge    L(byte32sse2_pre)
      .  
 14,039  	 add    %r8,%rdi
      .  #ifndef PIC
      .  	 lea    L(SSExDx)(%rip),%r9
      .  	 jmpq   *(%r9,%r8,8)
      .  #else
 14,039  	 lea    L(SSE0Q0)(%rip),%r9
 14,039  	 lea    L(SSExDx)(%rip),%rcx
 14,039  	 movswq (%rcx,%r8,2),%rcx
 14,039  	 lea    (%rcx,%r9,1),%r9
 14,039  	 jmpq   *%r9
      .  #endif
      .  
      .  L(SSE0QB):  movdqa %xmm0,-0xb0(%rdi)
      .  L(SSE0QA):  movdqa %xmm0,-0xa0(%rdi)
      .  L(SSE0Q9):  movdqa %xmm0,-0x90(%rdi)
      .  L(SSE0Q8):  movdqa %xmm0,-0x80(%rdi)
      .  L(SSE0Q7):  movdqa %xmm0,-0x70(%rdi)
      .  L(SSE0Q6):  movdqa %xmm0,-0x60(%rdi)
-- line 627 ----------------------------------------
-- line 732 ----------------------------------------
      .  L(SSE7Q2):  movdqa %xmm0,-0x27(%rdi)
      .  L(SSE7Q1):  movdqa %xmm0,-0x17(%rdi)
      .  L(SSE7Q0):  mov    %edx,-0x7(%rdi)
      .  	mov    %dx,-0x3(%rdi)
      .  	mov    %dl,-0x1(%rdi)
      .  	retq
      .  
      .  L(SSE8QB):  movdqa %xmm0,-0xb8(%rdi)
    246  L(SSE8QA):  movdqa %xmm0,-0xa8(%rdi)
 14,039  L(SSE8Q9):  movdqa %xmm0,-0x98(%rdi)
 14,039  L(SSE8Q8):  movdqa %xmm0,-0x88(%rdi)
 15,208  L(SSE8Q7):  movdqa %xmm0,-0x78(%rdi)
 15,757  L(SSE8Q6):  movdqa %xmm0,-0x68(%rdi)
 73,995  L(SSE8Q5):  movdqa %xmm0,-0x58(%rdi)
190,133  L(SSE8Q4):  movdqa %xmm0,-0x48(%rdi)
210,440  L(SSE8Q3):  movdqa %xmm0,-0x38(%rdi)
210,445  L(SSE8Q2):  movdqa %xmm0,-0x28(%rdi)
210,961  L(SSE8Q1):  movdqa %xmm0,-0x18(%rdi)
233,021  L(SSE8Q0):  mov    %rdx,-0x8(%rdi)
233,021  	retq
      .  
      .  L(SSE9QB):  movdqa %xmm0,-0xb9(%rdi)
      .  L(SSE9QA):  movdqa %xmm0,-0xa9(%rdi)
      .  L(SSE9Q9):  movdqa %xmm0,-0x99(%rdi)
      .  L(SSE9Q8):  movdqa %xmm0,-0x89(%rdi)
      .  L(SSE9Q7):  movdqa %xmm0,-0x79(%rdi)
      .  L(SSE9Q6):  movdqa %xmm0,-0x69(%rdi)
      .  L(SSE9Q5):  movdqa %xmm0,-0x59(%rdi)
-- line 759 ----------------------------------------
-- line 858 ----------------------------------------
      .  	mov    %edx,-0x7(%rdi)
      .  	mov    %dx,-0x3(%rdi)
      .  	mov    %dl,-0x1(%rdi)
      .  	retq
      .  
      .  	.balign     16
      .  L(byte32sse2_pre):
      .  
218,982  	mov    __x86_64_shared_cache_size(%rip),%r9d  # The largest cache size
218,982  	cmp    %r9,%r8
437,964  	jg     L(sse2_nt_move_pre)
      .  	#jmp    L(byte32sse2)
      .  	.balign     16
      .  L(byte32sse2):
435,433  	lea    -0x80(%r8),%r8 # 128
435,433  	cmp    $0x80,%r8   # 128
435,433  	movdqa %xmm0,(%rdi)
435,433  	movdqa %xmm0,0x10(%rdi)
435,433  	movdqa %xmm0,0x20(%rdi)
435,433  	movdqa %xmm0,0x30(%rdi)
435,433  	movdqa %xmm0,0x40(%rdi)
435,433  	movdqa %xmm0,0x50(%rdi)
435,433  	movdqa %xmm0,0x60(%rdi)
435,433  	movdqa %xmm0,0x70(%rdi)
      .  
435,433  	lea    0x80(%rdi),%rdi
435,433  	jge    L(byte32sse2)
218,982  	add    %r8,%rdi
      .  #ifndef PIC
      .  	lea    L(SSExDx)(%rip),%r11
      .  	jmpq   *(%r11,%r8,8)
      .  #else
218,982  	lea    L(SSE0Q0)(%rip),%r11
218,982  	lea    L(SSExDx)(%rip),%rcx
218,982  	movswq (%rcx,%r8,2),%rcx
218,982  	lea    (%rcx,%r11,1),%r11
218,982  	jmpq   *%r11
      .  #endif
      .  
      .  	.balign     16
      .  L(sse2_nt_move_pre):
      .  	cmp    $0x0,%r9
      .  	je     L(byte32sse2)
      .  	jmp    L(sse2_nt_move)
      .  
-- line 902 ----------------------------------------
-- line 1206 ----------------------------------------
      .  #ifndef USE_MULTIARCH
      .  L(aligned_now):
      .  
      .  	 cmpl   $0x1,__x86_64_preferred_memory_instruction(%rip)
      .  	 jg     L(SSE_pre)
      .  #endif /* USE_MULTIARCH */
      .  
      .  L(8byte_move_try):
      1  	cmpq	__STOS_LOWER_BOUNDARY,%r8
      2  	jae	L(8byte_stos_try)
      .  
      .  	.balign     16
      .  L(8byte_move):
      1  	movq	%r8,%rcx
      1  	shrq	$7,%rcx
      2  	jz	L(8byte_move_skip)
      .  
      .  	.p2align 4
      .  
      .  L(8byte_move_loop):
      4  	decq	%rcx
      .  
      4  	movq	%rdx,    (%rdi)
      4  	movq	%rdx,  8 (%rdi)
      4  	movq	%rdx, 16 (%rdi)
      4  	movq	%rdx, 24 (%rdi)
      4  	movq	%rdx, 32 (%rdi)
      4  	movq	%rdx, 40 (%rdi)
      4  	movq	%rdx, 48 (%rdi)
      4  	movq	%rdx, 56 (%rdi)
      4  	movq	%rdx, 64 (%rdi)
      4  	movq	%rdx, 72 (%rdi)
      4  	movq	%rdx, 80 (%rdi)
      4  	movq	%rdx, 88 (%rdi)
      4  	movq	%rdx, 96 (%rdi)
      4  	movq	%rdx, 104 (%rdi)
      4  	movq	%rdx, 112 (%rdi)
      4  	movq	%rdx, 120 (%rdi)
      .  
      4  	leaq	128 (%rdi),%rdi
      .  
      4  	jnz     L(8byte_move_loop)
      .  
      .  L(8byte_move_skip):
      1  	andl	$127,%r8d
      1  	lea    	(%rdi,%r8,1),%rdi
      .  
      .  #ifndef PIC
      .  	lea    	L(setPxQx)(%rip),%r11
      .  	jmpq   	*(%r11,%r8,8) # old scheme remained for nonPIC
      .  #else
      1  	lea    	L(Got0)(%rip),%r11
      1  	lea	L(setPxQx)(%rip),%rcx
      1  	movswq	(%rcx,%r8,2),%rcx
      1  	lea    	(%rcx,%r11,1),%r11
      1  	jmpq   	*%r11
      .  #endif
      .  
      .  	.balign     16
      .  L(8byte_stos_try):
      .  	mov    __x86_64_shared_cache_size(%rip),%r9d // ck largest cache size
      .  	cmpq	%r8,%r9		// calculate the lesser of remaining
      .  	cmovaq	%r8,%r9		// bytes and largest cache size
      .  	jbe	L(8byte_stos)
-- line 1269 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: um.c
--------------------------------------------------------------------------------
         Ir 

-- line 62 ----------------------------------------
          .  
          .  /* ARRAY */
          .  
          .  static inline void Array_new(unsigned hint, size_t size);
          .  static inline void* Array_at(unsigned pos);
          .  static inline void Array_resize();
          .  static inline void Array_free();
          .  
          5  int main(int argc, char *argv[]) {
          .          FILE* program;
          2          if (argc == 2) {
          9                  program = fopen(argv[1], "r");
     61,175  => /usr/src/debug/glibc-2.12-2-gc4ccff1/libio/../libio/iofopen.c:fopen@@GLIBC_2.2.5 (1x)
      1,116  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          .          } else {
          .                  program = NULL;
          .          }
          2          if (!program) {
          .                  fprintf(stderr, "usage: ./um <file in um binary format>\n");
          .                  exit(1);
          .          }
          .  
          .          mem_init();
          .          load(program);
          .  
          .          run();
          .  
          7          fclose(program);
        397  => /usr/src/debug/glibc-2.12-2-gc4ccff1/libio/iofclose.c:fclose@@GLIBC_2.2.5 (1x)
      1,202  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          .          mem_free();
          7  }
          .  
          .  /* UM CYCLE */
          .  
          .  static inline void run() {
425,352,611          while(program_counter < program_length) {
          .                  execute();
170,141,044                  program_counter++;
          .          }
          .  }
          .  
          .  /* PROGRAM LOAD */
          .  
          .  static inline Word readWord(FILE* program) {
     30,110          Word to_return = 0;
    240,880          for(unsigned i = 0; i < sizeof(Word); i++) {
    602,204                  ((unsigned char *)&to_return)[sizeof(Word)-i-1] = getc(program);
  4,697,387  => /usr/src/debug/glibc-2.12-2-gc4ccff1/libio/getc.c:getc (120440x)
      1,135  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          .          }
          .          return to_return;
          .  }
          .  
          .  static inline unsigned wordsInFile(FILE* program) {
          9          fseek(program, 0, SEEK_END);
        304  => /usr/src/debug/glibc-2.12-2-gc4ccff1/libio/fseek.c:fseek (1x)
      1,141  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          8          unsigned fileLength = ftell(program);
        110  => /usr/src/debug/glibc-2.12-2-gc4ccff1/libio/ioftell.c:ftell (1x)
      1,272  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          5          fseek(program, 0, SEEK_SET);
        158  => /usr/src/debug/glibc-2.12-2-gc4ccff1/libio/fseek.c:fseek (1x)
          1          return (fileLength / sizeof(Word));
          .  }
          .  
          .  static inline void load(FILE* program) {
          1          program_length = wordsInFile(program);
          .          map(program_length);
          .  
    120,446          for(unsigned i = 0; i < program_length; i++) {
          .                  put_word(0, i, readWord(program));
          .          }        
          .  }
          .  
          .  
          .  /* MEMORY */
          .  
          .  static const int SEQ_SIZE = 1000;
-- line 129 ----------------------------------------
-- line 135 ----------------------------------------
          .  static inline SegmentID new_seg_id();
          .  static inline Word* allocate_segment(unsigned length);
          .  static inline void* get_segment(int seg_id);
          .  
          .  /* === IMPLEMENTATION === */
          .  
          .  static inline void mem_init() {
          .          Queue_new(SEQ_SIZE, sizeof(Word));
          1          seq_length = 0;
          .          Array_new(SEG_MAP_SIZE, sizeof(uintptr_t));
          1          seg_map_length = SEG_MAP_SIZE;
          1          memory.next_seg = 0;
          .  }
          .  
          .  static inline void mem_free() {
          .          unmap(0);
          .          Queue_free();
          .          Array_free();
          1          seq_length = 0;
          1          seg_map_length = 0;
          .  }
          .  
          .  static inline SegmentID map(unsigned length) {
          .          SegmentID seg_id = new_seg_id();
  4,244,499          if (seg_id == memory.seg_map.length) {
          .                  Array_resize();
          .          }
          .          Word** seg = get_segment(seg_id);
  4,244,505          if(!((*seg) = allocate_segment(length))) {
          .                  fprintf(stderr, "Machine is out of memory!\n");
          .                  exit(1);
          .          }
  1,414,835          (*seg)[0] = length;
          .          return seg_id;
          .  }
          .  
          .  static inline void unmap(SegmentID id) {
          .          Word** seg = get_segment(id);
  4,230,142          free(*seg);
      1,158  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
127,851,390  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:free (1410045x)
  1,410,046          *seg = NULL;
  2,820,090          if (id) {
          .                  Queue_push(id);
  1,410,045                  seq_length++;
          .          } 
          .  }
          .  
          .  static inline void duplicate(SegmentID from) {
          .          Word** seg1 = get_segment(from);
          .          unsigned seg1_length = (*seg1)[0];
          .          Word** seg2 = get_segment(0);
          .          free(*seg2);
-- line 185 ----------------------------------------
-- line 187 ----------------------------------------
          .          for(unsigned i = 0; i <= seg1_length; i++) {
          .                  (*seg2)[i] = (*seg1)[i];
          .          }
          .          program_length = (*seg2)[0];
          .  }
          .  
          .  static inline Word put_word(SegmentID id, unsigned offset, Word value) {
          .          Word** seg = get_segment(id);
 32,521,936          Word to_return = (*seg)[offset+1];
 81,063,960          (*seg)[offset+1] = value;
          .          return to_return;
          .  }
          .  
          .  static inline Word get_word(SegmentID id, unsigned offset) {
          .          Word** seg = get_segment(id);
267,447,564          return (*seg)[offset+1];
          .  }
          .  
          .  static inline SegmentID new_seg_id() {
          .          SegmentID seg_id;
  4,244,505          if(seq_length) {
          .                  seg_id = Queue_pop();
  2,787,572                  seq_length--;
          .          } else {
     21,049                  seg_id = memory.next_seg;
     63,146                  memory.next_seg++;
          .          }
          .          return seg_id;
          .  }
          .  
          .  static inline Word* allocate_segment(unsigned length) {
  7,074,179          return calloc(sizeof(Word)*(length+1), sizeof(Word));
        250  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:calloc (1x)
      1,210  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          .  }
          .  
          .  static inline void* get_segment(int seg_id) {
 78,383,910          return memory.seg_map.values + seg_id * memory.seg_map.size;
          .  }
          .  
          .  /* REGISTERS */
          .  
          .  static inline void register_store(unsigned id, Word value) {
127,208,498          if (id > 7) {
          .                  fprintf(stderr, "Register %d does not exist."
          .                          " Must be in [0, 7]!\n", id);
          .                  exit(1);
          .          }
 63,604,249          registers[id] = value;
          .  }
          .  
          .  static inline Word register_load(unsigned id) {
          .          if (id > 7) {
          .                  fprintf(stderr, "Register %d does not exist. Must be in"
          .                  " [0, 7]!\n", id);
          .                  exit(1);
          .          }
432,725,154          return registers[id];
          .  }
          .  
          .  /* IO */
          .  
          .  static inline Word read() {
          .          Word to_return = getchar();
          .  
          .          if(to_return != (Word)EOF && (to_return > 255)) {
-- line 250 ----------------------------------------
-- line 252 ----------------------------------------
          .                  (unsigned)to_return);
          .                  exit(1);
          .          }
          .  
          .          return to_return;
          .  }
          .  
          .  static inline Word write(Word to_write) {
        362          if(to_write > 255) {
          .                  fprintf(stderr, "You cannot write value %u!\n",
          .                  (unsigned)to_write);
          .                  exit(1);
          .          }
          .  
        181          Word to_return = putchar((char)to_write);
          .  
          .          return to_return;
          .  }
          .  
          .  /* EXECUTION */
          .  
          .  typedef struct {
          .          Word opcode;
-- line 274 ----------------------------------------
-- line 316 ----------------------------------------
          .          &exe_input,
          .          &exe_jump,
          .          &exe_loadval
          .  };
          .  
          .  static inline void execute() {
          .          Word packed = get_word(0, program_counter);
          .          UM_Instruction i = unpack(packed);
170,141,044          if (i.opcode <= 13) {
850,705,220                  (*(functions[i.opcode]))(i);
 46,424,417  => um.c:exe_jump (3571109x)
 28,269,360  => um.c:exe_add (3141040x)
 42,958,900  => um.c:exe_nand (4295890x)
 28,789,664  => um.c:exe_move (2746419x)
308,768,105  => um.c:exe_map (1414834x)
          3  => um.c:exe_halt (1x)
  1,703,360  => um.c:exe_divide (170336x)
145,806,732  => um.c:exe_segstore (16200748x)
     10,505  => um.c:exe_output (181x)
172,584,220  => um.c:exe_unmap (1410045x)
272,458,256  => um.c:exe_segload (19461304x)
260,429,600  => um.c:exe_loadval (32553700x)
    944,235  => um.c:exe_multiply (104915x)
          .          }
          .  }
          .  
          .  static inline UM_Instruction unpack(Word packed) {
          .          UM_Instruction i;
          .  
          .          i.opcode = unpack_bits(packed, 4, 28);
          .  
170,141,044          if (i.opcode < 13) {
          .                  i.rA = unpack_bits(packed, 3, 6);
          .                  i.rAv = register_load(i.rA);
          .                  i.rB = unpack_bits(packed, 3, 3);
          .                  i.rBv = register_load(i.rB);
          .                  i.rC = unpack_bits(packed, 3, 0);
          .                  i.rCv = register_load(i.rC);
          .          } else {
          .                  i.rA = unpack_bits(packed, 3, 25);
          .                  i.rAv = register_load(i.rA);
          .                  i.value = unpack_bits(packed, 25, 0);
          .          }
          .  
680,564,176          return i;
          .  }
          .  
          .  static inline Word unpack_bits(Word bitword, unsigned width, unsigned lsb) {
863,295,798          return bitword << (32 - (width + lsb)) >> (32 - width);
          .  }
          .  
  2,746,419  static void exe_move(UM_Instruction i) {
  8,239,257          if (i.rCv != 0) {
  4,924,460                  register_store(i.rA, i.rBv);
          .          }
  5,492,838  }
 19,461,304  static void exe_segload(UM_Instruction i) {
 19,461,304          register_store(i.rA, get_word(i.rBv, i.rCv));
 38,922,608  }
          .  static void exe_segstore(UM_Instruction i) {
          .          put_word(i.rAv, i.rBv, i.rCv);
          .  }
  3,141,040  static void exe_add(UM_Instruction i) {
  9,423,120          register_store(i.rA, (i.rBv+i.rCv));
  6,282,080  }
    104,915  static void exe_multiply(UM_Instruction i) {
    314,745          register_store(i.rA, (i.rBv*i.rCv));
    209,830  }
    170,336  static void exe_divide(UM_Instruction i) {
    681,344          register_store(i.rA, (i.rBv/i.rCv));
    340,672  }
  4,295,890  static void exe_nand(UM_Instruction i) {
 17,183,560          register_store(i.rA, ~(i.rBv & i.rCv));
  8,591,780  }
          .  static void exe_halt(UM_Instruction i) {
          .          (void)i;
          3          program_counter = program_length;
          .  }
  4,244,502  static void exe_map(UM_Instruction i) {
  1,414,834          SegmentID seg = map(i.rCv);
  1,414,834          register_store(i.rB, seg);
  5,659,336  }
  4,230,135  static void exe_unmap(UM_Instruction i) {
  1,410,045          unmap(i.rCv);
  5,640,180  }
        181  static void exe_output(UM_Instruction i) {
        181          write(i.rCv);
        181  }
          .  static void exe_input(UM_Instruction i) {
          .          register_store(i.rC, read());
          .  }
 14,284,436  static void exe_jump(UM_Instruction i) {
 10,713,327          program_counter = i.rCv - 1;
  7,142,218          if (i.rBv) {
          .                  duplicate(i.rBv);
          .          }
 14,284,436  }
 32,553,700  static void exe_loadval(UM_Instruction i) {
 65,107,400          register_store(i.rA, i.value);
 65,107,400  }
          .  
          .  /* QUEUE */
          .  
          .  static inline void Queue_new(unsigned hint, size_t size) {
          1          memory.unmapped_ids.head = 0;
          1          memory.unmapped_ids.length = 0;
          1          memory.unmapped_ids.array.length = hint;
          8          memory.unmapped_ids.array.values = malloc(size * hint);
      1,145  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
        174  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:malloc (1x)
          1          memory.unmapped_ids.array.size = size;
          .  }
          .  
          .  static inline Word Queue_pop() {
  6,968,930          if (memory.unmapped_ids.length == 0) return UM_NULL;
 11,150,288          Word x = memory.unmapped_ids.array.values[(memory.unmapped_ids.head%memory.unmapped_ids.array.length)];
  8,362,716          memory.unmapped_ids.head = (memory.unmapped_ids.head+1)%memory.unmapped_ids.array.length;
  2,787,572          --memory.unmapped_ids.length;
          .          return x;
          .  }
          .  
          .  static inline void Queue_push(Word value) {
          .          unsigned i;
  8,460,260          if (memory.unmapped_ids.length == memory.unmapped_ids.array.length) {
          .                  Queue_resize();
          .          }
  2,820,090          i = memory.unmapped_ids.length++;
  7,050,225          memory.unmapped_ids.array.values[((memory.unmapped_ids.head+i)%memory.unmapped_ids.array.length)] = value;
          .  }
          .  
          .  
          .  static inline void Queue_resize() {
          .          unsigned n = memory.unmapped_ids.array.length;
         10          memory.unmapped_ids.array.length *= SCALEFACTOR;
         25          memory.unmapped_ids.array.values = realloc(memory.unmapped_ids.array.values, memory.unmapped_ids.array.size*memory.unmapped_ids.array.length);
  1,012,596  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:realloc (5x)
         15          if (memory.unmapped_ids.head > 0) {
         10                  Word* old = &(memory.unmapped_ids.array.values)[memory.unmapped_ids.head];
         44                  memcpy(old+n, old, (n - memory.unmapped_ids.head)*memory.unmapped_ids.array.size);
      1,145  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
      6,403  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/string/../sysdeps/x86_64/memcpy.S:memcpy (5x)
         35                  memory.unmapped_ids.head += n;
          .          }
          .  }
          .  
          .  static inline void Queue_free() {
          3          free(memory.unmapped_ids.array.values);
    580,470  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:free (1x)
          .  }
          .  
          .  /* ARRAY */
          .  
          .  static inline void Array_new(unsigned hint, size_t size) {
          4          memory.seg_map.values = malloc(size * hint);
        174  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:malloc (1x)
          1          memory.seg_map.length = hint;
          1          memory.seg_map.size = size;
          .  }
          .  
          .  static inline void* Array_at(unsigned pos) {
          .          return memory.seg_map.values + (pos * memory.seg_map.size);
          .  }
          .  
          .  static inline void Array_resize() {
         10          memory.seg_map.length *= SCALEFACTOR;
         34          memory.seg_map.values = realloc(memory.seg_map.values, memory.seg_map.size*memory.seg_map.length);
     72,004  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:realloc (5x)
      1,092  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/elf/../sysdeps/x86_64/dl-trampoline.S:_dl_runtime_resolve (1x)
          .  }
          .  
          .  static inline void Array_free() {
          3          free(memory.seg_map.values);
         36  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:free (1x)
          .  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c
--------------------------------------------------------------------------------
        Ir 

-- line 2472 ----------------------------------------
         .  #else
         .  static void malloc_init_state(av) mstate av;
         .  #endif
         .  {
         .    int     i;
         .    mbinptr bin;
         .  
         .    /* Establish circular links for normal bins */
       255    for (i = 1; i < NBINS; ++i) {
         3      bin = bin_at(av,i);
       382      bin->fd = bin->bk = bin;
         .    }
         .  
         .  #if MORECORE_CONTIGUOUS
         3    if (av != &main_arena)
         .  #endif
         .      set_noncontiguous(av);
         .    if (av == &main_arena)
         3      set_max_fast(DEFAULT_MXFAST);
         2    av->flags |= FASTCHUNKS_BIT;
         .  
         2    av->top            = initial_top(av);
         .  }
         .  
         .  /*
         .     Other internal utilities operating on mstates
         .  */
         .  
         .  #if __STD_C
         .  static Void_t*  sYSMALLOc(INTERNAL_SIZE_T, mstate);
-- line 2501 ----------------------------------------
-- line 2984 ----------------------------------------
         .    char*           aligned_brk;    /* aligned offset into brk */
         .  
         .    mchunkptr       p;              /* the allocated/returned chunk */
         .    mchunkptr       remainder;      /* remainder from allocation */
         .    unsigned long   remainder_size; /* its size */
         .  
         .    unsigned long   sum;            /* for updating stats */
         .  
        69    size_t          pagemask  = mp_.pagesize - 1;
         .    bool            tried_mmap = false;
         .  
         .  
         .  #if HAVE_MMAP
         .  
         .    /*
         .      If have mmap, and the request size meets the mmap threshold, and
         .      the system supports mmap, and there are few enough currently
         .      allocated mmapped regions, try to directly map this request
         .      rather than expanding top.
         .    */
         .  
        52    if ((unsigned long)(nb) >= (unsigned long)(mp_.mmap_threshold) &&
         .        (mp_.n_mmaps < mp_.n_mmaps_max)) {
         .  
         .      char* mm;             /* return value from mmap call*/
         .  
         .    try_mmap:
         .      /*
         .        Round up size to nearest page.  For mmapped chunks, the overhead
         .        is one SIZE_SZ unit larger than for normal chunks, because there
         .        is no following chunk whose prev_size field could be used.
         .      */
         .  #if 1
         .      /* See the front_misalign handling below, for glibc there is no
         .         need for further alignments.  */
        10      size = (nb + SIZE_SZ + pagemask) & ~pagemask;
         .  #else
         .      size = (nb + SIZE_SZ + MALLOC_ALIGN_MASK + pagemask) & ~pagemask;
         .  #endif
         .      tried_mmap = true;
         .  
         .      /* Don't try if size wraps around 0 */
         4      if ((unsigned long)(size) > (unsigned long)(nb)) {
         .  
        16        mm = (char*)(MMAP(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE));
        12  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/misc/../sysdeps/unix/syscall-template.S:mmap (2x)
         .  
         6        if (mm != MAP_FAILED) {
         .  
         .  	/*
         .  	  The offset to the start of the mmapped region is stored
         .  	  in the prev_size field of the chunk. This allows us to adjust
         .  	  returned start address to meet alignment requirements here
         .  	  and in memalign(), and still be able to compute proper
         .  	  address argument for later munmap in free() and realloc().
         .  	*/
-- line 3038 ----------------------------------------
-- line 3049 ----------------------------------------
         .  	  p = (mchunkptr)(mm + correction);
         .  	  p->prev_size = correction;
         .  	  set_head(p, (size - correction) |IS_MMAPPED);
         .  	}
         .  	else
         .  #endif
         .  	  {
         .  	    p = (mchunkptr)mm;
         6  	    set_head(p, size|IS_MMAPPED);
         .  	  }
         .  
         .  	/* update statistics */
         .  
        10  	if (++mp_.n_mmaps > mp_.max_n_mmaps)
         2  	  mp_.max_n_mmaps = mp_.n_mmaps;
         .  
         4  	sum = mp_.mmapped_mem += size;
         4  	if (sum > (unsigned long)(mp_.max_mmapped_mem))
         2  	  mp_.max_mmapped_mem = sum;
         .  #ifdef NO_THREADS
         .  	sum += av->system_mem;
         .  	if (sum > (unsigned long)(mp_.max_total_mem))
         .  	  mp_.max_total_mem = sum;
         .  #endif
         .  
         .  	check_chunk(av, p);
         .  
        46  	return chunk2mem(p);
         .        }
         .      }
         .    }
         .  #endif
         .  
         .    /* Record incoming configuration of top */
         .  
         .    old_top  = av->top;
         .    old_size = chunksize(old_top);
        21    old_end  = (char*)(chunk_at_offset(old_top, old_size));
         .  
         .    brk = snd_brk = (char*)(MORECORE_FAILURE);
         .  
         .    /*
         .       If not the first time through, we require old_size to be
         .       at least MINSIZE and to have prev_inuse set.
         .    */
         .  
-- line 3094 ----------------------------------------
-- line 3101 ----------------------------------------
         .    assert((unsigned long)(old_size) < (unsigned long)(nb + MINSIZE));
         .  
         .  #ifndef ATOMIC_FASTBINS
         .    /* Precondition: all fastbins are consolidated */
         .    assert(!have_fastchunks(av));
         .  #endif
         .  
         .  
        63    if (av != &main_arena) {
         .  
         .      heap_info *old_heap, *heap;
         .      size_t old_heap_size;
         .  
         .      /* First try to extend the current heap. */
         .      old_heap = heap_for_ptr(old_top);
         .      old_heap_size = old_heap->size;
         .      if ((long) (MINSIZE + nb - old_size) > 0
-- line 3117 ----------------------------------------
-- line 3163 ----------------------------------------
         .        /* We can at least try to use to mmap memory.  */
         .        goto try_mmap;
         .  
         .    } else { /* av == main_arena */
         .  
         .  
         .    /* Request enough space for nb + pad + overhead */
         .  
       126    size = nb + mp_.top_pad + MINSIZE;
         .  
         .    /*
         .      If contiguous, we can subtract out existing space that we hope to
         .      combine with new space. We add it back later only if
         .      we don't actually get contiguous space.
         .    */
         .  
        21    if (contiguous(av))
         .      size -= old_size;
         .  
         .    /*
         .      Round to a multiple of page size.
         .      If MORECORE is not contiguous, this ensures that we only call it
         .      with whole-page arguments.  And if MORECORE is contiguous and
         .      this is not first time through, this preserves page-alignment of
         .      previous calls. Otherwise, we correct to page-align below.
         .    */
         .  
       147    size = (size + pagemask) & ~pagemask;
         .  
         .    /*
         .      Don't try to call MORECORE if argument is so big as to appear
         .      negative. Note that since mmap takes size_t arg, it may succeed
         .      below even if we cannot call MORECORE.
         .    */
         .  
        42    if (size > 0)
       126      brk = (char*)(MORECORE(size));
       981  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/morecore.c:__default_morecore (21x)
         .  
        84    if (brk != (char*)(MORECORE_FAILURE)) {
         .      /* Call the `morecore' hook if necessary.  */
        42      void (*hook) (void) = force_reg (__after_morecore_hook);
        42      if (__builtin_expect (hook != NULL, 0))
         .        (*hook) ();
         .    } else {
         .    /*
         .      If have mmap, try using it as a backup when MORECORE fails or
         .      cannot be used. This is worth doing on systems that have "holes" in
         .      address space, so sbrk cannot extend to give contiguous space, but
         .      space is available elsewhere.  Note that we ignore mmap max count
         .      and threshold limits, since the space will not be used as a
-- line 3212 ----------------------------------------
-- line 3218 ----------------------------------------
         .      if (contiguous(av))
         .        size = (size + old_size + pagemask) & ~pagemask;
         .  
         .      /* If we are relying on mmap as backup, then use larger units */
         .      if ((unsigned long)(size) < (unsigned long)(MMAP_AS_MORECORE_SIZE))
         .        size = MMAP_AS_MORECORE_SIZE;
         .  
         .      /* Don't try if size wraps around 0 */
         2      if ((unsigned long)(size) > (unsigned long)(nb)) {
         .  
         .        char *mbrk = (char*)(MMAP(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE));
         .  
         .        if (mbrk != MAP_FAILED) {
         .  
         .  	/* We do not need, and cannot use, another sbrk call to find end */
         .  	brk = mbrk;
         .  	snd_brk = brk + size;
-- line 3234 ----------------------------------------
-- line 3241 ----------------------------------------
         .  	*/
         .  	set_noncontiguous(av);
         .        }
         .      }
         .  #endif
         .    }
         .  
         .    if (brk != (char*)(MORECORE_FAILURE)) {
        42      if (mp_.sbrk_base == 0)
         1        mp_.sbrk_base = brk;
        63      av->system_mem += size;
         .  
         .      /*
         .        If MORECORE extends previous space, we can likewise extend top size.
         .      */
         .  
        82      if (brk == old_end && snd_brk == (char*)(MORECORE_FAILURE))
       100        set_head(old_top, (size + old_size) | PREV_INUSE);
         .  
         7      else if (contiguous(av) && old_size && brk < old_end) {
         .        /* Oops!  Someone else killed our space..  Can't touch anything.  */
         .        mutex_unlock(&av->mutex);
         .        malloc_printerr (3, "break adjusted to free malloc space", brk);
         .        mutex_lock(&av->mutex);
         .      }
         .  
         .      /*
         .        Otherwise, make adjustments:
-- line 3268 ----------------------------------------
-- line 3285 ----------------------------------------
         .  
         .      else {
         .        front_misalign = 0;
         .        end_misalign = 0;
         .        correction = 0;
         .        aligned_brk = brk;
         .  
         .        /* handle contiguous cases */
         2        if (contiguous(av)) {
         .  
         .  	/* Count foreign sbrk as system_mem.  */
         .  	if (old_size)
         .  	  av->system_mem += brk - old_end;
         .  
         .  	/* Guarantee alignment of first new chunk made from this space */
         .  
         .  	front_misalign = (INTERNAL_SIZE_T)chunk2mem(brk) & MALLOC_ALIGN_MASK;
         5  	if (front_misalign > 0) {
         .  
         .  	  /*
         .  	    Skip over some bytes to arrive at an aligned position.
         .  	    We don't need to specially mark these wasted front bytes.
         .  	    They will never be accessed anyway because
         .  	    prev_inuse of av->top (and any chunk created from its start)
         .  	    is always true after initialization.
         .  	  */
-- line 3310 ----------------------------------------
-- line 3313 ----------------------------------------
         .  	  aligned_brk += correction;
         .  	}
         .  
         .  	/*
         .  	  If this isn't adjacent to existing space, then we will not
         .  	  be able to merge with old_top space, so must add to 2nd request.
         .  	*/
         .  
         1  	correction += old_size;
         .  
         .  	/* Extend the end address to hit a page boundary */
         2  	end_misalign = (INTERNAL_SIZE_T)(brk + size + correction);
         4  	correction += ((end_misalign + pagemask) & ~pagemask) - end_misalign;
         .  
         .  	assert(correction >= 0);
         5  	snd_brk = (char*)(MORECORE(correction));
        27  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/morecore.c:__default_morecore (1x)
         .  
         .  	/*
         .  	  If can't allocate correction, try to at least find out current
         .  	  brk.  It might be enough to proceed without failing.
         .  
         .  	  Note that if second sbrk did NOT fail, we assume that space
         .  	  is contiguous with first sbrk. This is a safe assumption unless
         .  	  program is multithreaded but doesn't use locks and a foreign sbrk
         .  	  occurred between our first and second calls.
         .  	*/
         .  
         4  	if (snd_brk == (char*)(MORECORE_FAILURE)) {
         .  	  correction = 0;
         .  	  snd_brk = (char*)(MORECORE(0));
         .  	} else {
         .  	  /* Call the `morecore' hook if necessary.  */
         2  	  void (*hook) (void) = force_reg (__after_morecore_hook);
         2  	  if (__builtin_expect (hook != NULL, 0))
         .  	    (*hook) ();
         .  	}
         .        }
         .  
         .        /* handle non-contiguous cases */
         .        else {
         .  	/* MORECORE/mmap must correctly align */
         .  	assert(((unsigned long)chunk2mem(brk) & MALLOC_ALIGN_MASK) == 0);
-- line 3354 ----------------------------------------
-- line 3356 ----------------------------------------
         .  	/* Find out current end of memory */
         .  	if (snd_brk == (char*)(MORECORE_FAILURE)) {
         .  	  snd_brk = (char*)(MORECORE(0));
         .  	}
         .        }
         .  
         .        /* Adjust top based on results of second sbrk */
         .        if (snd_brk != (char*)(MORECORE_FAILURE)) {
         1  	av->top = (mchunkptr)aligned_brk;
         4  	set_head(av->top, (snd_brk - aligned_brk + correction) | PREV_INUSE);
         1  	av->system_mem += correction;
         .  
         .  	/*
         .  	  If not the first time through, we either have a
         .  	  gap due to foreign sbrk or a non-contiguous region.  Insert a
         .  	  double fencepost at old_top to prevent consolidation with space
         .  	  we don't own. These fenceposts are artificial chunks that are
         .  	  marked as inuse and are in any case too small to use.  We need
         .  	  two to make sizes and alignments work out.
         .  	*/
         .  
         2  	if (old_size != 0) {
         .  	  /*
         .  	     Shrink old_top to insert fenceposts, keeping size a
         .  	     multiple of MALLOC_ALIGNMENT. We know there is at least
         .  	     enough space in old_top to do this.
         .  	  */
         .  	  old_size = (old_size - 4*SIZE_SZ) & ~MALLOC_ALIGN_MASK;
         .  	  set_head(old_top, old_size | PREV_INUSE);
         .  
-- line 3385 ----------------------------------------
-- line 3414 ----------------------------------------
         .      if (sum > (unsigned long)(mp_.max_total_mem))
         .        mp_.max_total_mem = sum;
         .  #endif
         .  
         .    }
         .  
         .    } /* if (av !=  &main_arena) */
         .  
        63    if ((unsigned long)av->system_mem > (unsigned long)(av->max_system_mem))
        21      av->max_system_mem = av->system_mem;
         .    check_malloc_state(av);
         .  
         .    /* finally, do the allocation */
         .    p = av->top;
         .    size = chunksize(p);
         .  
         .    /* check that one of the above allocation paths succeeded */
        84    if ((unsigned long)(size) >= (unsigned long)(nb + MINSIZE)) {
         .      remainder_size = size - nb;
        21      remainder = chunk_at_offset(p, nb);
        21      av->top = remainder;
       189      set_head(p, nb | PREV_INUSE | (av != &main_arena ? NON_MAIN_ARENA : 0));
        63      set_head(remainder, remainder_size | PREV_INUSE);
         .      check_malloced_chunk(av, p, nb);
        21      return chunk2mem(p);
         .    }
         .  
         .    /* catch all failure paths */
         .    MALLOC_FAILURE_ACTION;
         .    return 0;
         .  }
         .  
         .  
-- line 3446 ----------------------------------------
-- line 3461 ----------------------------------------
         .  {
         .    long  top_size;        /* Amount of top-most memory */
         .    long  extra;           /* Amount to release */
         .    long  released;        /* Amount actually released */
         .    char* current_brk;     /* address returned by pre-check sbrk call */
         .    char* new_brk;         /* address returned by post-check sbrk call */
         .    size_t pagesz;
         .  
         2    pagesz = mp_.pagesize;
         .    top_size = chunksize(av->top);
         .  
         .    /* Release in pagesize units, keeping at least one page */
        14    extra = ((top_size - pad - MINSIZE + (pagesz-1)) / pagesz - 1) * pagesz;
         .  
         4    if (extra > 0) {
         .  
         .      /*
         .        Only proceed if end of memory is where we last set it.
         .        This avoids problems if there were foreign sbrk calls.
         .      */
         8      current_brk = (char*)(MORECORE(0));
        54  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/morecore.c:__default_morecore (2x)
         8      if (current_brk == (char*)(av->top) + top_size) {
         .  
         .        /*
         .  	Attempt to release memory. We ignore MORECORE return value,
         .  	and instead call again to find out where new end of memory is.
         .  	This avoids problems if first call releases less than we asked,
         .  	of if failure somehow altered brk value. (We could still
         .  	encounter problems if it altered brk in some very bad way,
         .  	but the only thing we can do is adjust anyway, which will cause
         .  	some downstream failure.)
         .        */
         .  
         6        MORECORE(-extra);
        98  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/morecore.c:__default_morecore (2x)
         .        /* Call the `morecore' hook if necessary.  */
         4        void (*hook) (void) = force_reg (__after_morecore_hook);
         4        if (__builtin_expect (hook != NULL, 0))
         .  	(*hook) ();
         4        new_brk = (char*)(MORECORE(0));
        54  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/morecore.c:__default_morecore (2x)
         .  
         4        if (new_brk != (char*)MORECORE_FAILURE) {
         .  	released = (long)(current_brk - new_brk);
         .  
         4  	if (released != 0) {
         .  	  /* Success. Adjust top. */
         2  	  av->system_mem -= released;
        10  	  set_head(av->top, (top_size - released) | PREV_INUSE);
         .  	  check_malloc_state(av);
         .  	  return 1;
         .  	}
         .        }
         .      }
         .    }
         .    return 0;
         .  }
-- line 3515 ----------------------------------------
-- line 3527 ----------------------------------------
         .    INTERNAL_SIZE_T size = chunksize(p);
         .  
         .    assert (chunk_is_mmapped(p));
         .  #if 0
         .    assert(! ((char*)p >= mp_.sbrk_base && (char*)p < mp_.sbrk_base + mp_.sbrked_mem));
         .    assert((mp_.n_mmaps > 0));
         .  #endif
         .  
         6    uintptr_t block = (uintptr_t) p - p->prev_size;
         2    size_t total_size = p->prev_size + size;
         .    /* Unfortunately we have to do the compilers job by hand here.  Normally
         .       we would test BLOCK and TOTAL-SIZE separately for compliance with the
         .       page size.  But gcc does not recognize the optimization possibility
         .       (in the moment at least) so we combine the two values into one before
         .       the bit test.  */
        12    if (__builtin_expect (((block | total_size) & (mp_.pagesize - 1)) != 0, 0))
         .      {
         .        malloc_printerr (check_action, "munmap_chunk(): invalid pointer",
         .  		       chunk2mem (p));
         .        return;
         .      }
         .  
         2    mp_.n_mmaps--;
         2    mp_.mmapped_mem -= total_size;
         .  
         4    int ret __attribute__ ((unused)) = munmap((char *)block, total_size);
        10  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/misc/../sysdeps/unix/syscall-template.S:munmap (2x)
         .  
         .    /* munmap returns non-zero on failure */
         .    assert(ret == 0);
         .  }
         .  
         .  #if HAVE_MREMAP
         .  
         .  static mchunkptr
-- line 3560 ----------------------------------------
-- line 3611 ----------------------------------------
         .  #endif /* HAVE_MREMAP */
         .  
         .  #endif /* HAVE_MMAP */
         .  
         .  /*------------------------ Public wrappers. --------------------------------*/
         .  
         .  Void_t*
         .  public_mALLOc(size_t bytes)
        20  {
         .    mstate ar_ptr;
         .    Void_t *victim;
         .  
         .    __malloc_ptr_t (*hook) (size_t, __const __malloc_ptr_t)
         8      = force_reg (__malloc_hook);
         8    if (__builtin_expect (hook != NULL, 0))
         2      return (*hook)(bytes, RETURN_ADDRESS (0));
    60,855  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/hooks.c:malloc_hook_ini (1x)
         .  
         6    arena_lookup(ar_ptr);
         .  #if 0
         .    // XXX We need double-word CAS and fastbins must be extended to also
         .    // XXX hold a generation counter for each entry.
         .    if (ar_ptr) {
         .      INTERNAL_SIZE_T nb;               /* normalized request size */
         .      checked_request2size(bytes, nb);
         .      if (nb <= get_max_fast ()) {
         .        long int idx = fastbin_index(nb);
-- line 3636 ----------------------------------------
-- line 3653 ----------------------------------------
         .  	if (__builtin_expect (perturb_byte, 0))
         .  	  alloc_perturb (p, bytes);
         .  	return p;
         .        }
         .      }
         .    }
         .  #endif
         .  
        24    arena_lock(ar_ptr, bytes);
         .    if(!ar_ptr)
         .      return 0;
        12    victim = _int_malloc(ar_ptr, bytes);
       278  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_malloc (2x)
         6    if(!victim) {
         .      /* Maybe the failure is due to running out of mmapped areas. */
         .      if(ar_ptr != &main_arena) {
         .        (void)mutex_unlock(&ar_ptr->mutex);
         .        ar_ptr = &main_arena;
         .        (void)mutex_lock(&ar_ptr->mutex);
         .        victim = _int_malloc(ar_ptr, bytes);
         .        (void)mutex_unlock(&ar_ptr->mutex);
         .      } else {
-- line 3673 ----------------------------------------
-- line 3678 ----------------------------------------
         .        ar_ptr = arena_get2(prev, bytes, true);
         .        if(ar_ptr) {
         .  	victim = _int_malloc(ar_ptr, bytes);
         .  	(void)mutex_unlock(&ar_ptr->mutex);
         .        }
         .  #endif
         .      }
         .    } else
        12      (void)mutex_unlock(&ar_ptr->mutex);
         .    assert(!victim || chunk_is_mmapped(mem2chunk(victim)) ||
         .  	 ar_ptr == arena_for_chunk(mem2chunk(victim)));
         .    return victim;
        22  }
         .  #ifdef libc_hidden_def
         .  libc_hidden_def(public_mALLOc)
         .  #endif
         .  
         .  void
         .  public_fREe(Void_t* mem)
         .  {
         .    mstate ar_ptr;
         .    mchunkptr p;                          /* chunk corresponding to mem */
         .  
         .    void (*hook) (__malloc_ptr_t, __const __malloc_ptr_t)
 2,820,098      = force_reg (__free_hook);
 2,820,098    if (__builtin_expect (hook != NULL, 0)) {
         .      (*hook)(mem, RETURN_ADDRESS (0));
         .      return;
         .    }
         .  
 2,820,098    if (mem == 0)                              /* free(0) has no effect */
         .      return;
         .  
 1,410,049    p = mem2chunk(mem);
         .  
         .  #if HAVE_MMAP
 4,230,147    if (chunk_is_mmapped(p))                       /* release mmapped memory. */
         .    {
         .      /* see if the dynamic brk/mmap threshold needs adjusting */
        14      if (!mp_.no_dyn_threshold
         .  	&& p->size > mp_.mmap_threshold
         .  	&& p->size <= DEFAULT_MMAP_THRESHOLD_MAX)
         .        {
         3  	mp_.mmap_threshold = chunksize (p);
         3  	mp_.trim_threshold = 2 * mp_.mmap_threshold;
         .        }
         .      munmap_chunk(p);
         .      return;
         .    }
         .  #endif
         .  
 4,230,141    ar_ptr = arena_for_chunk(p);
         .  #ifdef ATOMIC_FASTBINS
 2,820,094    _int_free(ar_ptr, p, 0);
107,281,269  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_free (1410047x)
         .  #else
         .  # if THREAD_STATS
         .    if(!mutex_trylock(&ar_ptr->mutex))
         .      ++(ar_ptr->stat_lock_direct);
         .    else {
         .      (void)mutex_lock(&ar_ptr->mutex);
         .      ++(ar_ptr->stat_lock_wait);
         .    }
-- line 3739 ----------------------------------------
-- line 3745 ----------------------------------------
         .  #endif
         .  }
         .  #ifdef libc_hidden_def
         .  libc_hidden_def (public_fREe)
         .  #endif
         .  
         .  Void_t*
         .  public_rEALLOc(Void_t* oldmem, size_t bytes)
        99  {
         .    mstate ar_ptr;
         .    INTERNAL_SIZE_T    nb;      /* padded request size */
         .  
         .    Void_t* newp;             /* chunk to return */
         .  
         .    __malloc_ptr_t (*hook) (__malloc_ptr_t, size_t, __const __malloc_ptr_t) =
        22      force_reg (__realloc_hook);
        22    if (__builtin_expect (hook != NULL, 0))
         2      return (*hook)(oldmem, bytes, RETURN_ADDRESS (0));
     1,658  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/hooks.c:realloc_hook_ini (1x)
         .  
         .  #if REALLOC_ZERO_BYTES_FREES
        20    if (bytes == 0 && oldmem != NULL) { public_fREe(oldmem); return 0; }
         .  #endif
         .  
         .    /* realloc of null is supposed to be same as malloc */
        20    if (oldmem == 0) return public_mALLOc(bytes);
         .  
         .    /* chunk corresponding to oldmem */
        10    const mchunkptr oldp    = mem2chunk(oldmem);
         .    /* its size */
        30    const INTERNAL_SIZE_T oldsize = chunksize(oldp);
         .  
         .    /* Little security check which won't hurt performance: the
         .       allocator never wrapps around at the end of the address space.
         .       Therefore we can exclude some size values which might appear
         .       here by accident or by "design" from some intruder.  */
        60    if (__builtin_expect ((uintptr_t) oldp > (uintptr_t) -oldsize, 0)
         .        || __builtin_expect (misaligned_chunk (oldp), 0))
         .      {
         .        malloc_printerr (check_action, "realloc(): invalid pointer", oldmem);
         .        return NULL;
         .      }
         .  
        80    checked_request2size(bytes, nb);
         .  
         .  #if HAVE_MMAP
        20    if (chunk_is_mmapped(oldp))
         .    {
         .      Void_t* newmem;
         .  
         .  #if HAVE_MREMAP
         .      newp = mremap_chunk(oldp, nb);
         .      if(newp) return chunk2mem(newp);
         .  #endif
         .      /* Note the extra SIZE_SZ overhead. */
-- line 3798 ----------------------------------------
-- line 3801 ----------------------------------------
         .      newmem = public_mALLOc(bytes);
         .      if (newmem == 0) return 0; /* propagate failure */
         .      MALLOC_COPY(newmem, oldmem, oldsize - 2*SIZE_SZ);
         .      munmap_chunk(oldp);
         .      return newmem;
         .    }
         .  #endif
         .  
        30    ar_ptr = arena_for_chunk(oldp);
         .  #if THREAD_STATS
         .    if(!mutex_trylock(&ar_ptr->mutex))
         .      ++(ar_ptr->stat_lock_direct);
         .    else {
         .      (void)mutex_lock(&ar_ptr->mutex);
         .      ++(ar_ptr->stat_lock_wait);
         .    }
         .  #else
        60    (void)mutex_lock(&ar_ptr->mutex);
         .  #endif
         .  
         .  #if !defined NO_THREADS && !defined PER_THREAD
         .    /* As in malloc(), remember this arena for the next allocation. */
         .    tsd_setspecific(arena_key, (Void_t *)ar_ptr);
         .  #endif
         .  
        60    newp = _int_realloc(ar_ptr, oldp, oldsize, nb);
 1,082,308  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_realloc (9x)
         .  
        40    (void)mutex_unlock(&ar_ptr->mutex);
         .    assert(!newp || chunk_is_mmapped(mem2chunk(newp)) ||
         .  	 ar_ptr == arena_for_chunk(mem2chunk(newp)));
         .  
        30    if (newp == NULL)
         .      {
         .        /* Try harder to allocate memory in other arenas.  */
         .        newp = public_mALLOc(bytes);
         .        if (newp != NULL)
         .  	{
         .  	  MALLOC_COPY (newp, oldmem, oldsize - SIZE_SZ);
         .  #ifdef ATOMIC_FASTBINS
         .  	  _int_free(ar_ptr, oldp, 0);
-- line 3840 ----------------------------------------
-- line 3851 ----------------------------------------
         .  # endif
         .  	  _int_free(ar_ptr, oldp);
         .  	  (void)mutex_unlock(&ar_ptr->mutex);
         .  #endif
         .  	}
         .      }
         .  
         .    return newp;
        97  }
         .  #ifdef libc_hidden_def
         .  libc_hidden_def (public_rEALLOc)
         .  #endif
         .  
         .  Void_t*
         .  public_mEMALIGn(size_t alignment, size_t bytes)
         .  {
         .    mstate ar_ptr;
-- line 3867 ----------------------------------------
-- line 4029 ----------------------------------------
         .    assert(!p || chunk_is_mmapped(mem2chunk(p)) ||
         .  	 ar_ptr == arena_for_chunk(mem2chunk(p)));
         .  
         .    return p;
         .  }
         .  
         .  Void_t*
         .  public_cALLOc(size_t n, size_t elem_size)
 9,903,845  {
         .    mstate av;
         .    mchunkptr oldtop, p;
         .    INTERNAL_SIZE_T bytes, sz, csz, oldtopsize;
         .    Void_t* mem;
         .    unsigned long clearsize;
         .    unsigned long nclears;
         .    INTERNAL_SIZE_T* d;
         .  
         .    /* size_t is unsigned so the behavior on overflow is defined.  */
 2,829,670    bytes = n * elem_size;
         .  #define HALF_INTERNAL_SIZE_T \
         .    (((INTERNAL_SIZE_T) 1) << (8 * sizeof (INTERNAL_SIZE_T) / 2))
 7,074,175    if (__builtin_expect ((n | elem_size) >= HALF_INTERNAL_SIZE_T, 0)) {
         .      if (elem_size != 0 && bytes / elem_size != n) {
         .        MALLOC_FAILURE_ACTION;
         .        return 0;
         .      }
         .    }
         .  
         .    __malloc_ptr_t (*hook) __MALLOC_PMT ((size_t, __const __malloc_ptr_t)) =
 2,829,670      force_reg (__malloc_hook);
 2,829,670    if (__builtin_expect (hook != NULL, 0)) {
         .      sz = bytes;
         .      mem = (*hook)(sz, RETURN_ADDRESS (0));
         .      if(mem == 0)
         .        return 0;
         .  #ifdef HAVE_MEMCPY
         .      return memset(mem, 0, sz);
         .  #else
         .      while(sz > 0) ((char*)mem)[--sz] = 0; /* rather inefficient */
         .      return mem;
         .  #endif
         .    }
         .  
         .    sz = bytes;
         .  
14,148,350    arena_get(av, sz);
         .    if(!av)
         .      return 0;
         .  
         .    /* Check if we hand out the top chunk, in which case there may be no
         .       need to clear. */
         .  #if MORECORE_CLEARS
 1,414,835    oldtop = top(av);
 2,829,670    oldtopsize = chunksize(top(av));
         .  #if MORECORE_CLEARS < 2
         .    /* Only newly allocated memory is guaranteed to be cleared.  */
         .    if (av == &main_arena &&
         .        oldtopsize < mp_.sbrk_base + av->max_system_mem - (char *)oldtop)
         .      oldtopsize = (mp_.sbrk_base + av->max_system_mem - (char *)oldtop);
         .  #endif
 4,244,505    if (av != &main_arena)
         .      {
         .        heap_info *heap = heap_for_ptr (oldtop);
         .        if (oldtopsize < (char *) heap + heap->mprotect_size - (char *) oldtop)
         .  	oldtopsize = (char *) heap + heap->mprotect_size - (char *) oldtop;
         .      }
         .  #endif
 5,659,340    mem = _int_malloc(av, sz);
98,374,718  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_malloc (1414835x)
         .  
         .  
         .    assert(!mem || chunk_is_mmapped(mem2chunk(mem)) ||
         .  	 av == arena_for_chunk(mem2chunk(mem)));
         .  
 2,829,670    if (mem == 0) {
         .      /* Maybe the failure is due to running out of mmapped areas. */
         .      if(av != &main_arena) {
         .        (void)mutex_unlock(&av->mutex);
         .        (void)mutex_lock(&main_arena.mutex);
         .        mem = _int_malloc(&main_arena, sz);
         .        (void)mutex_unlock(&main_arena.mutex);
         .      } else {
         .  #if USE_ARENAS
-- line 4110 ----------------------------------------
-- line 4115 ----------------------------------------
         .        if(av) {
         .  	mem = _int_malloc(av, sz);
         .  	(void)mutex_unlock(&av->mutex);
         .        }
         .  #endif
         .      }
         .      if (mem == 0) return 0;
         .    } else
 5,659,340      (void)mutex_unlock(&av->mutex);
 1,414,835    p = mem2chunk(mem);
         .  
         .    /* Two optional cases in which clearing not necessary */
         .  #if HAVE_MMAP
 4,244,505    if (chunk_is_mmapped (p))
         .      {
         3        if (__builtin_expect (perturb_byte, 0))
         .  	MALLOC_ZERO (mem, sz);
         .        return mem;
         .      }
         .  #endif
         .  
 1,414,834    csz = chunksize(p);
         .  
         .  #if MORECORE_CLEARS
 7,135,037    if (perturb_byte == 0 && (p == oldtop && csz > oldtopsize)) {
         .      /* clear only the bytes from non-freshly-sbrked memory */
         .      csz = oldtopsize;
         .    }
         .  #endif
         .  
         .    /* Unroll clear of <= 36 bytes (72 if 8byte sizes).  We know that
         .       contents have an odd number of INTERNAL_SIZE_T-sized words;
         .       minimally 3.  */
         .    d = (INTERNAL_SIZE_T*)mem;
 1,414,834    clearsize = csz - SIZE_SZ;
 2,829,668    nclears = clearsize / sizeof(INTERNAL_SIZE_T);
         .    assert(nclears >= 3);
         .  
 2,829,668    if (nclears > 9)
 2,401,940      MALLOC_ZERO(d, clearsize);
25,457,444  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/string/../sysdeps/x86_64/memset.S:__GI_memset (600485x)
         .  
         .    else {
   814,349      *(d+0) = 0;
   814,349      *(d+1) = 0;
   814,349      *(d+2) = 0;
 1,628,698      if (nclears > 4) {
   814,347        *(d+3) = 0;
   814,347        *(d+4) = 0;
 1,628,694        if (nclears > 6) {
   814,345  	*(d+5) = 0;
   814,345  	*(d+6) = 0;
 1,628,690  	if (nclears > 8) {
   811,170  	  *(d+7) = 0;
 1,622,340  	  *(d+8) = 0;
         .  	}
         .        }
         .      }
         .    }
         .  
         .    return mem;
12,733,515  }
         .  
         .  #ifndef _LIBC
         .  
         .  Void_t**
         .  public_iCALLOc(size_t n, size_t elem_size, Void_t** chunks)
         .  {
         .    mstate ar_ptr;
         .    Void_t** m;
-- line 4183 ----------------------------------------
-- line 4272 ----------------------------------------
         .  }
         .  
         .  /*
         .    ------------------------------ malloc ------------------------------
         .  */
         .  
         .  static Void_t*
         .  _int_malloc(mstate av, size_t bytes)
12,733,623  {
         .    INTERNAL_SIZE_T nb;               /* normalized request size */
         .    unsigned int    idx;              /* associated bin index */
         .    mbinptr         bin;              /* associated bin */
         .  
         .    mchunkptr       victim;           /* inspected/selected chunk */
         .    INTERNAL_SIZE_T size;             /* its size */
         .    int             victim_index;     /* its bin index */
         .  
-- line 4288 ----------------------------------------
-- line 4302 ----------------------------------------
         .      Convert request size to internal form by adding SIZE_SZ bytes
         .      overhead plus possibly more to obtain necessary alignment and/or
         .      to obtain a size of at least MINSIZE, the smallest allocatable
         .      size. Also, checked_request2size traps (returning 0) request sizes
         .      that are so large that they wrap around zero when padded and
         .      aligned.
         .    */
         .  
11,318,776    checked_request2size(bytes, nb);
         .  
         .    /*
         .      If the size qualifies as a fastbin, first check corresponding bin.
         .      This code is safe to execute even if av is not yet initialized, so we
         .      can try it without checking, which saves some time on this fast path.
         .    */
         .  
 2,829,694    if ((unsigned long)(nb) <= (unsigned long)(get_max_fast ())) {
 3,541,665      idx = fastbin_index(nb);
 3,354,871      mfastbinptr* fb = &fastbin (av, idx);
         .  #ifdef ATOMIC_FASTBINS
 1,180,555      mchunkptr pp = *fb;
         .      do
         .        {
         .  	victim = pp;
 4,535,426  	if (victim == NULL)
         .  	  break;
         .        }
 6,522,948      while ((pp = catomic_compare_and_exchange_val_acq (fb, victim->fd, victim))
 3,261,474  	   != victim);
         .  #else
         .      victim = *fb;
         .  #endif
         .      if (victim != 0) {
 5,435,790        if (__builtin_expect (fastbin_index (chunksize (victim)) != idx, 0))
         .  	{
         .  	  errstr = "malloc(): memory corruption (fast)";
         .  	errout:
         .  	  mutex_unlock(&av->mutex);
         .  	  malloc_printerr (check_action, errstr, chunk2mem (victim));
         .  	  mutex_lock(&av->mutex);
         .  	  return NULL;
         .  	}
-- line 4343 ----------------------------------------
-- line 4355 ----------------------------------------
         .    /*
         .      If a small request, check regular bin.  Since these "smallbins"
         .      hold one size each, no searching within bins is necessary.
         .      (For a large request, we need to wait until unsorted chunks are
         .      processed to find best fit. But for small ones, fits are exact
         .      anyway, so we can check now, which is faster.)
         .    */
         .  
   655,378    if (in_smallbin_range(nb)) {
   983,007      idx = smallbin_index(nb);
 1,310,676      bin = bin_at(av,idx);
         .  
   983,007      if ( (victim = last(bin)) != bin) {
   418,978        if (victim == 0) /* initialization check */
         7  	malloc_consolidate(av);
       670  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:malloc_consolidate (1x)
         .        else {
   209,488  	bck = victim->bk;
   418,976  	if (__builtin_expect (bck->fd != victim, 0))
         .  	  {
         .  	    errstr = "malloc(): smallbin double linked list corrupted";
         .  	    goto errout;
         .  	  }
   209,488  	set_inuse_bit_at_offset(victim, nb);
   209,488  	bin->bk = bck;
   209,488  	bck->fd = bin;
         .  
   628,464  	if (av != &main_arena)
         .  	  victim->size |= NON_MAIN_ARENA;
         .  	check_malloced_chunk(av, victim, nb);
         .  	void *p = chunk2mem(victim);
         .  	if (__builtin_expect (perturb_byte, 0))
         .  	  alloc_perturb (p, bytes);
         .  	return p;
         .        }
         .      }
-- line 4389 ----------------------------------------
-- line 4396 ----------------------------------------
         .       fragmentation problems normally associated with fastbins.
         .       Also, in practice, programs tend to have runs of either small or
         .       large requests, but less often mixtures, so consolidation is not
         .       invoked all that often in most programs. And the programs that
         .       it is called frequently in otherwise tend to fragment.
         .    */
         .  
         .    else {
       255      idx = largebin_index(nb);
        60      if (have_fastchunks(av))
        30        malloc_consolidate(av);
   934,283  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:malloc_consolidate (5x)
         .    }
         .  
         .    /*
         .      Process recently freed or remaindered chunks, taking one only if
         .      it is exact fit, or, if this a small request, the chunk is remainder from
         .      the most recent non-exact fit.  Place other traversed chunks in
         .      bins.  Note that this step is the only place in any routine where
         .      chunks are placed in bins.
-- line 4414 ----------------------------------------
-- line 4417 ----------------------------------------
         .      near the end of malloc that we should have consolidated, so must
         .      do so and retry. This happens at most once, and only when we would
         .      otherwise need to expand memory to service a "small" request.
         .    */
         .  
         .    for(;;) {
         .  
         .      int iters = 0;
 1,251,077      while ( (victim = unsorted_chunks(av)->bk) != unsorted_chunks(av)) {
   313,126        bck = victim->bk;
 1,252,504        if (__builtin_expect (victim->size <= 2 * SIZE_SZ, 0)
   313,126  	  || __builtin_expect (victim->size > av->system_mem, 0))
         .  	{
         .  	  void *p = chunk2mem(victim);
         .  	  mutex_unlock(&av->mutex);
         .  	  malloc_printerr (check_action, "malloc(): memory corruption", p);
   118,201  	  mutex_lock(&av->mutex);
         .  	}
   313,126        size = chunksize(victim);
         .  
         .        /*
         .  	 If a small request, try to use last remainder if it is the
         .  	 only chunk in unsorted bin.  This helps promote locality for
         .  	 runs of consecutive small requests. This is the only
         .  	 exception to best-fit, and applies only when there is
         .  	 no exact fit for a small chunk.
         .        */
         .  
 1,618,210        if (in_smallbin_range(nb) &&
         .  	  bck == unsorted_chunks(av) &&
    92,076  	  victim == av->last_remainder &&
   236,402  	  (unsigned long)(size) > (unsigned long)(nb + MINSIZE)) {
         .  
         .  	/* split and reattach remainder */
    88,298  	remainder_size = size - nb;
   176,596  	remainder = chunk_at_offset(victim, nb);
   176,596  	unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
    88,298  	av->last_remainder = remainder;
   176,596  	remainder->bk = remainder->fd = unsorted_chunks(av);
   176,596  	if (!in_smallbin_range(remainder_size))
         .  	  {
    81,786  	    remainder->fd_nextsize = NULL;
    81,786  	    remainder->bk_nextsize = NULL;
         .  	  }
         .  
   706,384  	set_head(victim, nb | PREV_INUSE |
         .  		 (av != &main_arena ? NON_MAIN_ARENA : 0));
   264,894  	set_head(remainder, remainder_size | PREV_INUSE);
    88,298  	set_foot(remainder, remainder_size);
         .  
         .  	check_malloced_chunk(av, victim, nb);
    88,298  	void *p = chunk2mem(victim);
   353,192  	if (__builtin_expect (perturb_byte, 0))
         .  	  alloc_perturb (p, bytes);
         .  	return p;
         .        }
         .  
         .        /* remove from unsorted list */
   224,828        unsorted_chunks(av)->bk = bck;
   224,828        bck->fd = unsorted_chunks(av);
         .  
         .        /* Take now instead of binning if exact fit */
         .  
   449,656        if (size == nb) {
     4,808  	set_inuse_bit_at_offset(victim, size);
    14,424  	if (av != &main_arena)
         .  	  victim->size |= NON_MAIN_ARENA;
         .  	check_malloced_chunk(av, victim, nb);
         .  	void *p = chunk2mem(victim);
         .  	if (__builtin_expect (perturb_byte, 0))
         .  	  alloc_perturb (p, bytes);
         .  	return p;
         .        }
         .  
         .        /* place chunk in bin */
         .  
   440,040        if (in_smallbin_range(size)) {
   219,196  	victim_index = smallbin_index(size);
   657,588  	bck = bin_at(av, victim_index);
 1,753,568  	fwd = bck->fd;
         .        }
         .        else {
    14,846  	victim_index = largebin_index(size);
       824  	bck = bin_at(av, victim_index);
       824  	fwd = bck->fd;
         .  
         .  	/* maintain large bins in sorted order */
     1,648  	if (fwd != bck) {
         .  	  /* Or with inuse bit to speed comparisons */
       595  	  size |= PREV_INUSE;
         .  	  /* if smaller than smallest, bypass loop below */
         .  	  assert((bck->bk->size & NON_MAIN_ARENA) == 0);
     1,785  	  if ((unsigned long)(size) < (unsigned long)(bck->bk->size)) {
         .  	    fwd = bck;
         .  	    bck = bck->bk;
         .  
        36  	    victim->fd_nextsize = fwd->fd;
        72  	    victim->bk_nextsize = fwd->fd->bk_nextsize;
       144  	    fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
         .  	  }
         .  	  else {
         .  	    assert((fwd->size & NON_MAIN_ARENA) == 0);
     2,607  	    while ((unsigned long) size < fwd->size)
         .  	      {
       310  		fwd = fwd->fd_nextsize;
         .  		assert((fwd->size & NON_MAIN_ARENA) == 0);
         .  	      }
         .  
     1,118  	    if ((unsigned long) size == (unsigned long) fwd->size)
         .  	      /* Always insert in the second position.  */
       944  	      fwd = fwd->fd;
         .  	    else
         .  	      {
        87  		victim->fd_nextsize = fwd;
       174  		victim->bk_nextsize = fwd->bk_nextsize;
        87  		fwd->bk_nextsize = victim;
       174  		victim->bk_nextsize->fd_nextsize = victim;
         .  	      }
     1,118  	    bck = fwd->bk;
         .  	  }
         .  	} else
       916  	  victim->fd_nextsize = victim->bk_nextsize = victim;
         .        }
         .  
   440,040        mark_bin(av, victim_index);
   220,020        victim->bk = bck;
   220,020        victim->fd = fwd;
   220,020        fwd->bk = victim;
   440,040        bck->fd = victim;
         .  
         .  #define MAX_ITERS	10000
   440,040        if (++iters >= MAX_ITERS)
         .  	break;
         .      }
         .  
         .      /*
         .        If a large request, scan through the chunks of current bin in
         .        sorted order to find smallest that fits.  Use the skip list for this.
         .      */
         .  
    75,294      if (!in_smallbin_range(nb)) {
        76        bin = bin_at(av, idx);
         .  
         .        /* skip scan if empty or largest chunk is too small */
        61        if ((victim = first(bin)) != bin &&
         .  	  (unsigned long)(victim->size) >= (unsigned long)(nb)) {
         .  
         .  	victim = victim->bk_nextsize;
         .  	while (((unsigned long)(size = chunksize(victim)) <
         .  		(unsigned long)(nb)))
         .  	  victim = victim->bk_nextsize;
         .  
         .  	/* Avoid removing the first entry for a size so that the skip
-- line 4569 ----------------------------------------
-- line 4620 ----------------------------------------
         .        (with ties going to approximately the least recently used) chunk
         .        that fits is selected.
         .  
         .        The bitmap avoids needing to check that most blocks are nonempty.
         .        The particular case of skipping all bins during warm-up phases
         .        when no chunks have been returned yet is faster than it might look.
         .      */
         .  
    50,196      ++idx;
   100,392      bin = bin_at(av,idx);
    50,196      block = idx2block(idx);
    50,196      map = av->binmap[block];
   100,392      bit = idx2bit(idx);
         .  
         .      for (;;) {
         .  
         .        /* Skip rest of block if there are no more set bits in this block.  */
   128,787        if (bit > map || bit == 0) {
         .  	do {
   250,362  	  if (++block >= BINMAPSIZE)  /* out of bins */
         .  	    goto use_top;
   252,608  	} while ( (map = av->binmap[block]) == 0);
         .  
     5,718  	bin = bin_at(av, (block << BINMAPSHIFT));
         .  	bit = 1;
         .        }
         .  
         .        /* Advance to bin with set bit. There must be one. */
    58,103        while ((bit & map) == 0) {
    22,007  	bin = next_bin(bin);
    22,007  	bit <<= 1;
         .  	assert(bit != 0);
         .        }
         .  
         .        /* Inspect the bin. It is likely to be non-empty */
     6,590        victim = last(bin);
         .  
         .        /*  If a false alarm (empty bin), clear the bit. */
    13,180        if (victim == bin) {
     7,176  	av->binmap[block] = map &= ~bit; /* Write through */
     1,794  	bin = next_bin(bin);
     1,794  	bit <<= 1;
         .        }
         .  
         .        else {
     9,592  	size = chunksize(victim);
         .  
         .  	/*  We know the first chunk in this bin is big enough to use. */
         .  	assert((unsigned long)(size) >= (unsigned long)(nb));
         .  
     9,592  	remainder_size = size - nb;
         .  
         .  	/* unlink */
    52,382  	unlink(av, victim, bck, fwd);
         .  
         .  	/* Exhaust */
     9,592  	if (remainder_size < MINSIZE) {
     1,047  	  set_inuse_bit_at_offset(victim, size);
     4,188  	  if (av != &main_arena)
         .  	    victim->size |= NON_MAIN_ARENA;
         .  	}
         .  
         .  	/* Split */
         .  	else {
     3,749  	  remainder = chunk_at_offset(victim, nb);
         .  
         .  	  /* We cannot assume the unsorted list is empty and therefore
         .  	     have to perform a complete insert here.  */
         .  	  bck = unsorted_chunks(av);
     3,749  	  fwd = bck->fd;
     7,498  	  if (__builtin_expect (fwd->bk != bck, 0))
         .  	    {
         .  	      errstr = "malloc(): corrupted unsorted chunks 2";
         .  	      goto errout;
         .  	    }
     3,749  	  remainder->bk = bck;
     3,749  	  remainder->fd = fwd;
     3,749  	  bck->fd = remainder;
     3,749  	  fwd->bk = remainder;
         .  
         .  	  /* advertise as last remainder */
     7,498  	  if (in_smallbin_range(nb))
     3,747  	    av->last_remainder = remainder;
     7,498  	  if (!in_smallbin_range(remainder_size))
         .  	    {
       767  	      remainder->fd_nextsize = NULL;
       767  	      remainder->bk_nextsize = NULL;
         .  	    }
    29,992  	  set_head(victim, nb | PREV_INUSE |
         .  		   (av != &main_arena ? NON_MAIN_ARENA : 0));
    14,996  	  set_head(remainder, remainder_size | PREV_INUSE);
     3,749  	  set_foot(remainder, remainder_size);
         .  	}
         .  	check_malloced_chunk(av, victim, nb);
 1,306,250  	void *p = chunk2mem(victim);
 3,918,750  	if (__builtin_expect (perturb_byte, 0))
         .  	  alloc_perturb (p, bytes);
         .  	return p;
         .        }
     1,794      }
         .  
         .    use_top:
         .      /*
         .        If large enough, split off the chunk bordering the end of memory
         .        (held in av->top). Note that this is in accord with the best-fit
         .        search rule.  In effect, av->top is treated as larger (and thus
         .        less well fitting) than any other available chunk since it can
         .        be extended to be as large as necessary (up to system
-- line 4727 ----------------------------------------
-- line 4729 ----------------------------------------
         .  
         .        We require that av->top always exists (i.e., has size >=
         .        MINSIZE) after initialization, so if it would otherwise be
         .        exhausted by current request, it is replenished. (The main
         .        reason for ensuring it exists is that we may need MINSIZE space
         .        to put in fenceposts in sysmalloc.)
         .      */
         .  
    40,604      victim = av->top;
    40,604      size = chunksize(victim);
         .  
    40,604      if ((unsigned long)(size) >= (unsigned long)(nb + MINSIZE)) {
         .        remainder_size = size - nb;
    20,276        remainder = chunk_at_offset(victim, nb);
    20,276        av->top = remainder;
   182,484        set_head(victim, nb | PREV_INUSE |
         .  	       (av != &main_arena ? NON_MAIN_ARENA : 0));
    81,104        set_head(remainder, remainder_size | PREV_INUSE);
         .  
         .        check_malloced_chunk(av, victim, nb);
    20,276        void *p = chunk2mem(victim);
    81,196        if (__builtin_expect (perturb_byte, 0))
         .  	alloc_perturb (p, bytes);
         .        return p;
         .      }
         .  
         .  #ifdef ATOMIC_FASTBINS
         .      /* When we are using atomic ops to free fast chunks we can get
         .         here for all block sizes.  */
        52      else if (have_fastchunks(av)) {
        15        malloc_consolidate(av);
     2,765  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:malloc_consolidate (3x)
         .        /* restore original bin index */
        15        if (in_smallbin_range(nb))
   354,603  	idx = smallbin_index(nb);
         .        else
 3,191,439  	idx = largebin_index(nb);
         .      }
         .  #else
         .      /*
         .        If there is space available in fastbins, consolidate and retry,
         .        to possibly avoid expanding memory. This can occur only if nb is
         .        in smallbin range so we didn't consolidate upon entry.
         .      */
         .  
-- line 4772 ----------------------------------------
-- line 4777 ----------------------------------------
         .      }
         .  #endif
         .  
         .      /*
         .         Otherwise, relay to handle system-dependent cases
         .      */
         .      else {
         .        void *p = sYSMALLOc(nb, av);
        69        if (p != NULL && __builtin_expect (perturb_byte, 0))
        43  	alloc_perturb (p, bytes);
         .        return p;
         .      }
         .    }
12,733,623  }
         .  
         .  /*
         .    ------------------------------ free ------------------------------
         .  */
         .  
         .  static void
         .  #ifdef ATOMIC_FASTBINS
         .  _int_free(mstate av, mchunkptr p, int have_lock)
         .  #else
         .  _int_free(mstate av, mchunkptr p)
         .  #endif
14,100,560  {
         .    INTERNAL_SIZE_T size;        /* its size */
         .    mfastbinptr*    fb;          /* associated fastbin */
         .    mchunkptr       nextchunk;   /* next contiguous chunk */
         .    INTERNAL_SIZE_T nextsize;    /* its size */
         .    int             nextinuse;   /* true if nextchunk is used */
         .    INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
         .    mchunkptr       bck;         /* misc temp for linking */
         .    mchunkptr       fwd;         /* misc temp for linking */
         .  
         .    const char *errstr = NULL;
         .  #ifdef ATOMIC_FASTBINS
         .    int locked = 0;
         .  #endif
         .  
 4,230,168    size = chunksize(p);
         .  
         .    /* Little security check which won't hurt performance: the
         .       allocator never wrapps around at the end of the address space.
         .       Therefore we can exclude some size values which might appear
         .       here by accident or by "design" from some intruder.  */
 8,460,336    if (__builtin_expect ((uintptr_t) p > (uintptr_t) -size, 0)
         .        || __builtin_expect (misaligned_chunk (p), 0))
         .      {
         .        errstr = "free(): invalid pointer";
         .      errout:
         .  #ifdef ATOMIC_FASTBINS
         .        if (have_lock || locked)
         .  	(void)mutex_unlock(&av->mutex);
         .  #endif
-- line 4831 ----------------------------------------
-- line 4832 ----------------------------------------
         .        malloc_printerr (check_action, errstr, chunk2mem(p));
         .  #ifdef ATOMIC_FASTBINS
         .        if (have_lock)
         .  	mutex_lock(&av->mutex);
         .  #endif
         .        return;
         .      }
         .    /* We know that each chunk is at least MINSIZE bytes in size.  */
 2,820,112    if (__builtin_expect (size < MINSIZE, 0))
         .      {
         .        errstr = "free(): invalid size";
         .        goto errout;
         .      }
         .  
         .    check_inuse_chunk(av, p);
         .  
         .    /*
         .      If eligible, place chunk on a fastbin so it can be found
         .      and used quickly in malloc.
         .    */
         .  
 2,820,112    if ((unsigned long)(size) <= (unsigned long)(get_max_fast ())
         .  
         .  #if TRIM_FASTBINS
         .        /*
         .  	If TRIM_FASTBINS set, don't place chunks
         .  	bordering top into fastbins
         .        */
         .        && (chunk_at_offset(p, size) != av->top)
         .  #endif
         .        ) {
         .  
 5,881,970      if (__builtin_expect (chunk_at_offset (p, size)->size <= 2 * SIZE_SZ, 0)
 2,352,788  	|| __builtin_expect (chunksize (chunk_at_offset (p, size))
         .  			     >= av->system_mem, 0))
         .        {
         .  #ifdef ATOMIC_FASTBINS
         .  	/* We might not have a lock at this point and concurrent modifications
         .  	   of system_mem might have let to a false positive.  Redo the test
         .  	   after getting the lock.  */
         .  	if (have_lock
         .  	    || ({ assert (locked == 0);
-- line 4873 ----------------------------------------
-- line 4885 ----------------------------------------
         .  	if (! have_lock)
         .  	  {
         .  	    (void)mutex_unlock(&av->mutex);
         .  	    locked = 0;
         .  	  }
         .  #endif
         .        }
         .  
 3,529,182      if (__builtin_expect (perturb_byte, 0))
         .        free_perturb (chunk2mem(p), size - SIZE_SZ);
         .  
 3,529,182      set_fastchunks(av);
 2,352,788      unsigned int idx = fastbin_index(size);
 2,352,788      fb = &fastbin (av, idx);
         .  
         .  #ifdef ATOMIC_FASTBINS
         .      /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
 1,176,394      mchunkptr old = *fb, old2;
         .      unsigned int old_idx = ~0u;
         .      do
         .        {
         .  	/* Check that the top of the bin is not the record we are going to add
         .  	   (i.e., double free).  */
 8,234,758  	if (__builtin_expect (old == p, 0))
         .  	  {
         .  	    errstr = "double free or corruption (fasttop)";
         .  	    goto errout;
         .  	  }
         .  	/* Check that size of fastbin chunk at the top is the same as
         .  	   size of the chunk that we are adding.  We can dereference OLD
         .  	   only if we have the lock, otherwise it might have already been
         .  	   deallocated.  See use of OLD_IDX below for the actual check.  */
 4,705,576  	if (have_lock && old != NULL)
         .  	  old_idx = fastbin_index(chunksize(old));
 1,176,394  	p->fd = old2 = old;
         .        }
 7,058,364      while ((old = catomic_compare_and_exchange_val_rel (fb, p, old2)) != old2);
         .  
 2,352,788      if (have_lock && old != NULL && __builtin_expect (old_idx != idx, 0))
         .        {
         .  	errstr = "invalid fastbin entry (free)";
         .  	goto errout;
         .        }
         .  #else
         .      /* Another simple check: make sure the top of the bin is not the
         .         record we are going to add (i.e., double free).  */
         .      if (__builtin_expect (*fb == p, 0))
-- line 4931 ----------------------------------------
-- line 4944 ----------------------------------------
         .      *fb = p;
         .  #endif
         .    }
         .  
         .    /*
         .      Consolidate other non-mmapped chunks as they arrive.
         .    */
         .  
   467,324    else if (!chunk_is_mmapped(p)) {
         .  #ifdef ATOMIC_FASTBINS
   934,648      if (! have_lock) {
         .  # if THREAD_STATS
         .        if(!mutex_trylock(&av->mutex))
         .  	++(av->stat_lock_direct);
         .        else {
         .  	(void)mutex_lock(&av->mutex);
         .  	++(av->stat_lock_wait);
         .        }
         .  # else
 1,869,224        (void)mutex_lock(&av->mutex);
         .  # endif
         .        locked = 1;
         .      }
         .  #endif
         .  
   233,662      nextchunk = chunk_at_offset(p, size);
         .  
         .      /* Lightweight tests: check whether the block is already the
         .         top block.  */
   700,986      if (__builtin_expect (p == av->top, 0))
         .        {
         .  	errstr = "double free or corruption (top)";
         .  	goto errout;
         .        }
         .      /* Or whether the next chunk is beyond the boundaries of the arena.  */
 1,168,310      if (__builtin_expect (contiguous (av)
         .  			  && (char *) nextchunk
   700,986  			  >= ((char *) av->top + chunksize(av->top)), 0))
         .        {
         .  	errstr = "double free or corruption (out)";
         .  	goto errout;
         .        }
         .      /* Or whether the block is actually not marked used.  */
   700,986      if (__builtin_expect (!prev_inuse(nextchunk), 0))
         .        {
         .  	errstr = "double free or corruption (!prev)";
         .  	goto errout;
         .        }
         .  
   233,662      nextsize = chunksize(nextchunk);
   934,648      if (__builtin_expect (nextchunk->size <= 2 * SIZE_SZ, 0)
         .  	|| __builtin_expect (nextsize >= av->system_mem, 0))
         .        {
         .  	errstr = "free(): invalid next size (normal)";
         .  	goto errout;
         .        }
         .  
   700,986      if (__builtin_expect (perturb_byte, 0))
         .        free_perturb (chunk2mem(p), size - SIZE_SZ);
         .  
         .      /* consolidate backward */
   467,324      if (!prev_inuse(p)) {
     1,208        prevsize = p->prev_size;
     1,208        size += prevsize;
     1,208        p = chunk_at_offset(p, -((long) prevsize));
    14,014        unlink(av, p, bck, fwd);
         .      }
         .  
   467,324      if (nextchunk != av->top) {
         .        /* get and clear inuse bit */
         .        nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
         .  
         .        /* consolidate forward */
   467,314        if (!nextinuse) {
    35,845  	unlink(av, nextchunk, bck, fwd);
     3,228  	size += nextsize;
         .        } else
   460,858  	clear_inuse_bit_at_offset(nextchunk, 0);
         .  
         .        /*
         .  	Place the chunk in unsorted chunk list. Chunks are
         .  	not placed into regular bins until after they have
         .  	been given one chance to be used in malloc.
         .        */
         .  
   233,657        bck = unsorted_chunks(av);
   233,657        fwd = bck->fd;
   467,314        if (__builtin_expect (fwd->bk != bck, 0))
         .  	{
         .  	  errstr = "free(): corrupted unsorted chunks";
         .  	  goto errout;
         .  	}
   233,657        p->fd = fwd;
   233,657        p->bk = bck;
   467,314        if (!in_smallbin_range(size))
         .  	{
     1,884  	  p->fd_nextsize = NULL;
     1,884  	  p->bk_nextsize = NULL;
         .  	}
   233,657        bck->fd = p;
   233,657        fwd->bk = p;
         .  
   700,971        set_head(p, size | PREV_INUSE);
   233,657        set_foot(p, size);
         .  
         .        check_free_chunk(av, p);
         .      }
         .  
         .      /*
         .        If the chunk borders the current high end of memory,
         .        consolidate into top
         .      */
         .  
         .      else {
         5        size += nextsize;
        20        set_head(p, size | PREV_INUSE);
         5        av->top = p;
         .        check_chunk(av, p);
         .      }
         .  
         .      /*
         .        If freeing a large space, consolidate possibly-surrounding
         .        chunks. Then, if the total unused topmost memory exceeds trim
         .        threshold, ask malloc_trim to reduce top.
         .  
-- line 5068 ----------------------------------------
-- line 5069 ----------------------------------------
         .        Unless max_fast is 0, we don't know if there are fastbins
         .        bordering top, so we cannot tell for sure whether threshold
         .        has been reached unless fastbins are consolidated.  But we
         .        don't want to consolidate on each free.  As a compromise,
         .        consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
         .        is reached.
         .      */
         .  
   467,324      if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
       168        if (have_fastchunks(av))
       249  	malloc_consolidate(av);
 2,923,173  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:malloc_consolidate (83x)
         .  
       252        if (av == &main_arena) {
         .  #ifndef MORECORE_CANNOT_TRIM
       420  	if ((unsigned long)(chunksize(av->top)) >=
         .  	    (unsigned long)(mp_.trim_threshold))
         .  	  sYSTRIm(mp_.top_pad, av);
         .  #endif
         .        } else {
         .  	/* Always try heap_trim(), even if the top chunk is not
         .  	   large, because the corresponding heap might go away.  */
         .  	heap_info *heap = heap_for_ptr(top(av));
         .  
         .  	assert(heap->ar_ptr == av);
         .  	heap_trim(heap, mp_.top_pad);
         .        }
         .      }
         .  
         .  #ifdef ATOMIC_FASTBINS
   700,986      if (! have_lock) {
         .        assert (locked);
   934,612        (void)mutex_unlock(&av->mutex);
         .      }
         .  #endif
         .    }
         .    /*
         .      If the chunk was allocated via mmap, release via munmap(). Note
         .      that if HAVE_MMAP is false but chunk_is_mmapped is true, then
         .      user must have overwritten memory. There's nothing we can do to
         .      catch this error unless MALLOC_DEBUG is set, in which case
-- line 5108 ----------------------------------------
-- line 5109 ----------------------------------------
         .      check_inuse_chunk (above) will have triggered error.
         .    */
         .  
         .    else {
         .  #if HAVE_MMAP
         .      munmap_chunk (p);
         .  #endif
         .    }
11,280,448  }
         .  
         .  /*
         .    ------------------------- malloc_consolidate -------------------------
         .  
         .    malloc_consolidate is a specialized version of free() that tears
         .    down chunks held in fastbins.  Free itself cannot be used for this
         .    purpose since, among other things, it might place chunks back onto
         .    fastbins.  So, instead, we need to use a minor variant of the same
-- line 5125 ----------------------------------------
-- line 5126 ----------------------------------------
         .    code.
         .  
         .    Also, because this routine needs to be called the first time through
         .    malloc anyway, it turns out to be the perfect place to trigger
         .    initialization code.
         .  */
         .  
         .  #if __STD_C
         2  static void malloc_consolidate(mstate av)
         .  #else
         .  static void malloc_consolidate(av) mstate av;
         .  #endif
       736  {
         .    mfastbinptr*    fb;                 /* current fastbin being consolidated */
         .    mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
         .    mchunkptr       p;                  /* current chunk being consolidated */
         .    mchunkptr       nextp;              /* next chunk to consolidate */
         .    mchunkptr       unsorted_bin;       /* bin header */
         .    mchunkptr       first_unsorted;     /* chunk to link to */
         .  
         .    /* These have same use as in free() */
-- line 5146 ----------------------------------------
-- line 5152 ----------------------------------------
         .    mchunkptr       bck;
         .    mchunkptr       fwd;
         .  
         .    /*
         .      If max_fast is 0, we know that av hasn't
         .      yet been initialized, in which case do so below
         .    */
         .  
       184    if (get_max_fast () != 0) {
       273      clear_fastchunks(av);
         .  
        91      unsorted_bin = unsorted_chunks(av);
         .  
         .      /*
         .        Remove each chunk from fast bin and consolidate it, placing it
         .        then in unsorted bin. Among other reasons for doing this,
         .        placing in unsorted bin avoids needing to calculate actual bins
         .        until malloc is sure that chunks aren't immediately going to be
         .        reused anyway.
         .      */
-- line 5171 ----------------------------------------
-- line 5172 ----------------------------------------
         .  
         .  #if 0
         .      /* It is wrong to limit the fast bins to search using get_max_fast
         .         because, except for the main arena, all the others might have
         .         blocks in the high fast bins.  It's not worth it anyway, just
         .         search all bins all the time.  */
         .      maxfb = &fastbin (av, fastbin_index(get_max_fast ()));
         .  #else
        91      maxfb = &fastbin (av, NFASTBINS - 1);
         .  #endif
       182      fb = &fastbin (av, 0);
         .      do {
         .  #ifdef ATOMIC_FASTBINS
     1,820        p = atomic_exchange_acq (fb, 0);
         .  #else
         .        p = *fb;
         .  #endif
     2,473        if (p != 0) {
         .  #ifndef ATOMIC_FASTBINS
         .  	*fb = 0;
         .  #endif
         .  	do {
         .  	  check_inuse_chunk(av, p);
    89,236  	  nextp = p->fd;
         .  
         .  	  /* Slightly streamlined version of consolidation code in free() */
   267,708  	  size = p->size & ~(PREV_INUSE|NON_MAIN_ARENA);
    89,236  	  nextchunk = chunk_at_offset(p, size);
   178,472  	  nextsize = chunksize(nextchunk);
         .  
   178,472  	  if (!prev_inuse(p)) {
    61,478  	    prevsize = p->prev_size;
    61,478  	    size += prevsize;
    61,478  	    p = chunk_at_offset(p, -((long) prevsize));
   665,968  	    unlink(av, p, bck, fwd);
         .  	  }
         .  
   178,472  	  if (nextchunk != av->top) {
         .  	    nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
         .  
   178,454  	    if (!nextinuse) {
    40,289  	      size += nextsize;
   411,024  	      unlink(av, nextchunk, bck, fwd);
         .  	    } else
    97,876  	      clear_inuse_bit_at_offset(nextchunk, 0);
         .  
    89,227  	    first_unsorted = unsorted_bin->fd;
    89,227  	    unsorted_bin->fd = p;
    89,227  	    first_unsorted->bk = p;
         .  
   178,454  	    if (!in_smallbin_range (size)) {
    20,652  	      p->fd_nextsize = NULL;
    20,652  	      p->bk_nextsize = NULL;
         .  	    }
         .  
   267,681  	    set_head(p, size | PREV_INUSE);
    89,227  	    p->bk = unsorted_bin;
    89,227  	    p->fd = first_unsorted;
    89,227  	    set_foot(p, size);
         .  	  }
         .  
         .  	  else {
         .  	    size += nextsize;
        36  	    set_head(p, size | PREV_INUSE);
    89,245  	    av->top = p;
         .  	  }
         .  
   178,472  	} while ( (p = nextp) != 0);
         .  
         .        }
     3,458      } while (fb++ != maxfb);
         .    }
         .    else {
         .      malloc_init_state(av);
         .      check_malloc_state(av);
         .    }
       736  }
         .  
         .  /*
         .    ------------------------------ realloc ------------------------------
         .  */
         .  
         .  Void_t*
         .  _int_realloc(mstate av, mchunkptr oldp, INTERNAL_SIZE_T oldsize,
         .  	     INTERNAL_SIZE_T nb)
        90  {
         .    mchunkptr        newp;            /* chunk to return */
         .    INTERNAL_SIZE_T  newsize;         /* its size */
         .    Void_t*          newmem;          /* corresponding user mem */
         .  
         .    mchunkptr        next;            /* next contiguous chunk after oldp */
         .  
         .    mchunkptr        remainder;       /* extra space at end of newp */
         .    unsigned long    remainder_size;  /* its size */
-- line 5265 ----------------------------------------
-- line 5270 ----------------------------------------
         .    unsigned long    copysize;        /* bytes to copy */
         .    unsigned int     ncopies;         /* INTERNAL_SIZE_T words to copy */
         .    INTERNAL_SIZE_T* s;               /* copy source */
         .    INTERNAL_SIZE_T* d;               /* copy destination */
         .  
         .    const char *errstr = NULL;
         .  
         .    /* oldmem size */
        40    if (__builtin_expect (oldp->size <= 2 * SIZE_SZ, 0)
        30        || __builtin_expect (oldsize >= av->system_mem, 0))
         .      {
         .        errstr = "realloc(): invalid old size";
         .      errout:
         .        mutex_unlock(&av->mutex);
         .        malloc_printerr (check_action, errstr, chunk2mem(oldp));
         .        mutex_lock(&av->mutex);
         .        return NULL;
         .      }
-- line 5287 ----------------------------------------
-- line 5291 ----------------------------------------
         .    /* All callers already filter out mmap'ed chunks.  */
         .  #if 0
         .    if (!chunk_is_mmapped(oldp))
         .  #else
         .    assert (!chunk_is_mmapped(oldp));
         .  #endif
         .    {
         .  
        10      next = chunk_at_offset(oldp, oldsize);
        20      INTERNAL_SIZE_T nextsize = chunksize(next);
        40      if (__builtin_expect (next->size <= 2 * SIZE_SZ, 0)
         .  	|| __builtin_expect (nextsize >= av->system_mem, 0))
         .        {
         .  	errstr = "realloc(): invalid next size";
         .  	goto errout;
         .        }
         .  
        20      if ((unsigned long)(oldsize) >= (unsigned long)(nb)) {
         .        /* already big enough; split below */
         .        newp = oldp;
         .        newsize = oldsize;
         .      }
         .  
         .      else {
         .        /* Try to expand forward into top */
        23        if (next == av->top &&
         .  	  (unsigned long)(newsize = oldsize + nextsize) >=
         1  	  (unsigned long)(nb + MINSIZE)) {
         9  	set_head_size(oldp, nb | (av != &main_arena ? NON_MAIN_ARENA : 0));
         2  	av->top = chunk_at_offset(oldp, nb);
         3  	set_head(av->top, (newsize - nb) | PREV_INUSE);
         .  	check_inuse_chunk(av, oldp);
         2  	return chunk2mem(oldp);
         .        }
         .  
         .        /* Try to expand forward into next chunk;  split off remainder below */
        21        else if (next != av->top &&
         .  	       !inuse(next) &&
         .  	       (unsigned long)(newsize = oldsize + nextsize) >=
         .  	       (unsigned long)(nb)) {
         .  	newp = oldp;
         .  	unlink(av, next, bck, fwd);
         .        }
         .  
         .        /* allocate, copy, free */
         .        else {
        45  	newmem = _int_malloc(av, nb - MALLOC_ALIGN_MASK);
   985,392  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_malloc (9x)
        36  	if (newmem == 0)
         .  	  return 0; /* propagate failure */
         .  
         9  	newp = mem2chunk(newmem);
         9  	newsize = chunksize(newp);
         .  
         .  	/*
         .  	  Avoid copy if newp is next chunk after oldp.
         .  	*/
        18  	if (newp == next) {
         .  	  newsize += oldsize;
         .  	  newp = oldp;
         .  	}
         .  	else {
         .  	  /*
         .  	    Unroll copy of <= 36 bytes (72 if 8byte sizes)
         .  	    We know that contents have an odd number of
         .  	    INTERNAL_SIZE_T-sized words; minimally 3.
         .  	  */
         .  
         9  	  copysize = oldsize - SIZE_SZ;
         9  	  s = (INTERNAL_SIZE_T*)(chunk2mem(oldp));
         .  	  d = (INTERNAL_SIZE_T*)(newmem);
        18  	  ncopies = copysize / sizeof(INTERNAL_SIZE_T);
         .  	  assert(ncopies >= 3);
         .  
        18  	  if (ncopies > 9)
        27  	    MALLOC_COPY(d, s, copysize);
    96,979  => /usr/src/debug////////glibc-2.12-2-gc4ccff1/string/../sysdeps/x86_64/memcpy.S:memcpy (9x)
         .  
         .  	  else {
         .  	    *(d+0) = *(s+0);
         .  	    *(d+1) = *(s+1);
         .  	    *(d+2) = *(s+2);
         .  	    if (ncopies > 4) {
         .  	      *(d+3) = *(s+3);
         .  	      *(d+4) = *(s+4);
-- line 5373 ----------------------------------------
-- line 5378 ----------------------------------------
         .  		  *(d+7) = *(s+7);
         .  		  *(d+8) = *(s+8);
         .  		}
         .  	      }
         .  	    }
         .  	  }
         .  
         .  #ifdef ATOMIC_FASTBINS
        36  	  _int_free(av, oldp, 1);
       852  => /usr/src/debug/glibc-2.12-2-gc4ccff1/malloc/malloc.c:_int_free (9x)
         .  #else
         .  	  _int_free(av, oldp);
         .  #endif
         .  	  check_inuse_chunk(av, newp);
        28  	  return chunk2mem(newp);
         .  	}
         .        }
         .      }
         .  
         .      /* If possible, free extra space in old or extended chunk */
         .  
         .      assert((unsigned long)(newsize) >= (unsigned long)(nb));
         .  
-- line 5399 ----------------------------------------
-- line 5486 ----------------------------------------
         .  #else
         .      /* If !HAVE_MMAP, but chunk_is_mmapped, user must have overwritten mem */
         .      check_malloc_state(av);
         .      MALLOC_FAILURE_ACTION;
         .      return 0;
         .  #endif
         .    }
         .  #endif
        70  }
         .  
         .  /*
         .    ------------------------------ memalign ------------------------------
         .  */
         .  
         .  static Void_t*
         .  _int_memalign(mstate av, size_t alignment, size_t bytes)
         .  {
-- line 5502 ----------------------------------------

--------------------------------------------------------------------------------
 Ir 
--------------------------------------------------------------------------------
100  percentage of events annotated

